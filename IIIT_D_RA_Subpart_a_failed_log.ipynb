{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# My journey. My thinkings and my workprocess to tackle the given problem"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here in this notebook, I have kept a step by step analysis on my working. So as all I started by importing the then necessary modules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import cv2\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But sice this is a CNN model so then realised that the manditory modules must be included. Initially was confused whether to proceed with tflearn API or Keras API or use simple tensorflow.\n",
    "Next step was opening the documentation of each module mentioned above\n",
    "Keras: https://keras.io/guides/functional_api/\n",
    "Tflearn: http://tflearn.org/\n",
    "Tensorflow: https://www.tensorflow.org/guide\n",
    "\n",
    "Next read the doc provided by IIIT_D. Decided to proceed with Keras as I was most comfortable with it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:523: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:524: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:526: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:527: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "D:\\anaconda\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:532: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from tensorflow import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense, Dropout, Activation, Flatten\n",
    "from keras.layers import Convolution2D,Conv2D, MaxPooling2D\n",
    "from keras.utils import np_utils"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we required our dataset to be imported hence, need the os module. Also to make my importing of images professional let us use tqdm module. Lastly the king of python numerical calculation module namely numpy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from tqdm import tqdm\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First try for importing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "path=\"D://datasets//IIIT_D//train\";\n",
    "fol=os.listdir(path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "But wait let me first check on one image to see its dimensions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b4588f9b0>"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=cv2.imread(\"D://datasets//IIIT_D//train//Sample001//img001-008.png\",cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now went to my folder where the dataset was downloaded as to check is the picture coloured or not. Although even if it was coloured, I have used Grayscale\n",
    "while importing hence, this coloured background made no sense. It quickly struck me that the imshow() in matplotlib and cv2 have different default colouing layers.\n",
    "Thus, searched on google as to how to print this on matplotlib. Found the documentation of matplotlib - https://matplotlib.org/stable/tutorials/colors/colormaps.html"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b45936cf8>"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=cv2.imread(\"D://datasets//IIIT_D//train//Sample001//img001-008.png\",cv2.IMREAD_GRAYSCALE)\n",
    "plt.imshow(img,cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that the shade looks ok, I checked the dimension of the image"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(900, 1200)\n"
     ]
    }
   ],
   "source": [
    "print(img.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The ratio of the image is 3:4 So, initially tried with (150,200) where 150:200=3:4. Hence, just need to reshape our image that way and check if the image still has all the important data points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b47bf62e8>"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=cv2.imread(\"D://datasets//IIIT_D//train//Sample058//img058-008.png\",cv2.IMREAD_GRAYSCALE)/255\n",
    "img1=cv2.resize(img,(150,200))\n",
    "plt.imshow(img1,cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Orientation distorted so lets check with 200,150"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.image.AxesImage at 0x24b47d46d68>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "img=cv2.imread(\"D://datasets//IIIT_D//train//Sample058//img058-008.png\",cv2.IMREAD_GRAYSCALE)/255\n",
    "img1=cv2.resize(img,(200,150))\n",
    "plt.imshow(img1,cmap=plt.cm.binary)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok good to go. So next I defined the empty lists and imported the images there "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Next Step: Accessing the dataset from the local machine in X and y lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "0it [00:00, ?it/s]\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'one_hot' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-10-d9e63f6022c1>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      6\u001b[0m         \u001b[0mimg\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mimread\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjoin\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_n\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mimg_n\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mIMREAD_GRAYSCALE\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m/\u001b[0m\u001b[1;36m255\u001b[0m\u001b[1;31m#normalize by dividing by 255 for better results.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      7\u001b[0m         \u001b[0mX\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mcv2\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mresize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mimg\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m200\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m150\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#initially we obtained a 2d array. In order to fit it in our keras CNN model, we had to convert it into a 3d model. Also the image is downsized to avoid OOM (Out of Memory) exception\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 8\u001b[1;33m         \u001b[0my\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mone_hot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mval\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0msize\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;31m#store our results by converting it into a one-hot vector. Here the enumerate () returning counter val palys a very important role.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'one_hot' is not defined"
     ]
    }
   ],
   "source": [
    "X,y=list(),list();\n",
    "size=len(fol);\n",
    "for val,i in tqdm(enumerate(fol)):\n",
    "    path_n=os.path.join(path,i);\n",
    "    for img_n in os.listdir(path_n):\n",
    "        img=cv2.imread(os.path.join(path_n,img_n),cv2.IMREAD_GRAYSCALE)/255#normalize by dividing by 255 for better results.\n",
    "        X.append((np.reshape(cv2.resize(img,(200,150)),(200,150,1))))#initially we obtained a 2d array. In order to fit it in our keras CNN model, we had to convert it into a 3d model. Also the image is downsized to avoid OOM (Out of Memory) exception\n",
    "        y.append(one_hot(val,size))#store our results by converting it into a one-hot vector. Here the enumerate () returning counter val palys a very important role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok now that we have our dataset imported let us move to define our one_hot code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(val,size):\n",
    "    a=np.array(np.zeros((1,size)));\n",
    "    a[0][val]=1;\n",
    "    return a;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [01:52,  1.88s/it]\n"
     ]
    }
   ],
   "source": [
    "X,y=list(),list();\n",
    "size=len(fol);\n",
    "for val,i in tqdm(enumerate(fol)):\n",
    "    path_n=os.path.join(path,i);\n",
    "    for img_n in os.listdir(path_n):\n",
    "        img=cv2.imread(os.path.join(path_n,img_n),cv2.IMREAD_GRAYSCALE)/255#normalize by dividing by 255 for better results.\n",
    "        X.append((np.reshape(cv2.resize(img,(200,150)),(200,150,1))))#initially we obtained a 2d array. In order to fit it in our keras CNN model, we had to convert it into a 3d model. Also the image is downsized to avoid OOM (Out of Memory) exception\n",
    "        y.append(one_hot(val,size))#store our results by converting it into a one-hot vector. Here the enumerate () returning counter val palys a very important role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok so we are ready to get our model for training the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(248, kernel_size=5, activation=\"relu\", input_shape=(200, 150,..., padding=\"same\")`\n",
      "  \n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(62, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 200, 150, 248)     6448      \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 200, 150, 62)      384462    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 100, 75, 62)       0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 100, 75, 62)       0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 465000)            0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 62)                28830062  \n",
      "=================================================================\n",
      "Total params: 29,220,972\n",
      "Trainable params: 29,220,972\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(248, kernel_size=5, activation='relu',border_mode='same', input_shape=(200,150,1)))\n",
    "model.add(Convolution2D(62, (5, 5), activation='relu',border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(62, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I was overconscious about the effect of overfitting and underfitting so wanted to start with small networks and if failed then move on to biggere networks. As a result our this model failed\n",
    "More of a disaster if one wanna say."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'X_train_arr' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-d512df2c8e22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_arr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_arr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_arr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'X_train_arr' is not defined"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history=model.fit(x=X_train_arr,y=y_train_arr,epochs=10,validation_data=(X_val_arr,y_val_arr))\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.15, random_state=42)\n",
    "X_train_arr=np.array(X_train)\n",
    "X_val_arr=np.array(X_val)\n",
    "y_train_arr=np.array(y_train)\n",
    "y_val_arr=np.array(y_val)\n",
    "_,h,w,d=X_train_arr.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "Error when checking target: expected dense_1 to have 2 dimensions, but got array with shape (2108, 1, 62)",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-d512df2c8e22>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcompile\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mloss\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'categorical_crossentropy'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0moptimizer\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'adam'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mmetrics\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'accuracy'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mhistory\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mmodel\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mx\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mX_train_arr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0my_train_arr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mepochs\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m10\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mX_val_arr\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0my_val_arr\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhistory\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36mfit\u001b[1;34m(self, x, y, batch_size, epochs, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, steps_per_epoch, validation_steps, **kwargs)\u001b[0m\n\u001b[0;32m    950\u001b[0m             \u001b[0msample_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    951\u001b[0m             \u001b[0mclass_weight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 952\u001b[1;33m             batch_size=batch_size)\n\u001b[0m\u001b[0;32m    953\u001b[0m         \u001b[1;31m# Prepare validation data.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    954\u001b[0m         \u001b[0mdo_validation\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training.py\u001b[0m in \u001b[0;36m_standardize_user_data\u001b[1;34m(self, x, y, sample_weight, class_weight, check_array_lengths, batch_size)\u001b[0m\n\u001b[0;32m    787\u001b[0m                 \u001b[0mfeed_output_shapes\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    788\u001b[0m                 \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m  \u001b[1;31m# Don't enforce the batch size.\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 789\u001b[1;33m                 exception_prefix='target')\n\u001b[0m\u001b[0;32m    790\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    791\u001b[0m             \u001b[1;31m# Generate sample-wise weight values given the `sample_weight` and\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mD:\\anaconda\\lib\\site-packages\\keras\\engine\\training_utils.py\u001b[0m in \u001b[0;36mstandardize_input_data\u001b[1;34m(data, names, shapes, check_batch_axis, exception_prefix)\u001b[0m\n\u001b[0;32m    126\u001b[0m                         \u001b[1;34m': expected '\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mnames\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mi\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' to have '\u001b[0m \u001b[1;33m+\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    127\u001b[0m                         \u001b[0mstr\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mshape\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' dimensions, but got array '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 128\u001b[1;33m                         'with shape ' + str(data_shape))\n\u001b[0m\u001b[0;32m    129\u001b[0m                 \u001b[1;32mif\u001b[0m \u001b[1;32mnot\u001b[0m \u001b[0mcheck_batch_axis\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    130\u001b[0m                     \u001b[0mdata_shape\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdata_shape\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: Error when checking target: expected dense_1 to have 2 dimensions, but got array with shape (2108, 1, 62)"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',optimizer='adam',metrics=['accuracy'])\n",
    "history=model.fit(x=X_train_arr,y=y_train_arr,epochs=10,validation_data=(X_val_arr,y_val_arr))\n",
    "print(history.history)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Well I tried a lot but could not remove this error on the first two days. I was totally focussing on the fact that this error was involved with my training images's dimensions.\n",
    "I chenged the dimensions from (200,150,1) to (1,150,200). Obviously this did not work. It does nothing but reshape the image in some other axis.\n",
    "It was giving me 3 dimensions.\n",
    "I start calculating\n",
    "X= 4 dimensional array(2108,200,150,1)-----> passed on 1st layer\n",
    "number of parameters in this layer = 1x200x150x248=7440000 weights----> output dimension(2108,200,150,248)\n",
    "----> passed into second layer\n",
    "number of parameters here=200x150x248x62=461,280,000--->output dimension(2108,200,150,62).\n",
    "Passed it through max_2d_pooling layer---->(2,2) kernel so here we create a 2x2 window over the previous matrix and pull the maximum of all\n",
    "---->output dimension =(2108,200/2,150/2,62)=(2108,100,75,62)---->Flatten to (2108,62) each image mapped to a straight vector.\n",
    "----> Then pass onto Dense layer which is actually a fully connected layer to previous layer. So dimension definitely will not change\n",
    "So our input files are giving (2108,62) as dimension and not (2108,1,62) as dimension. I was completely lost here.\n",
    "\n",
    "\n",
    "After through checking I understood that this was my output one_hot vector dimensions which was clashing and if one sees my this version of one hot vector()---> it declares a numpy array of zeroes of (1,62) which is a 2D array and then for 2108 times if we accumulate this value, it will give me a 3D array (2108,1,62). Hence, Now I had to change the one_hot() completely (as shown below)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# New version of one_hot()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "def one_hot(val,size):\n",
    "    a=np.array([0]*size);\n",
    "    a[val]=1;\n",
    "    return a;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After this, I rerun and reload the dataset. Then finally running in 3 different models to find the better output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "62it [01:45,  1.66s/it]\n"
     ]
    }
   ],
   "source": [
    "X,y=list(),list();\n",
    "for val,i in tqdm(enumerate(fol)):\n",
    "    path_n=os.path.join(path,i);\n",
    "    for img_n in os.listdir(path_n):\n",
    "        img=cv2.imread(os.path.join(path_n,img_n),cv2.IMREAD_GRAYSCALE)/255#normalize by dividing by 255 for better results.\n",
    "        X.append((np.reshape(cv2.resize(img,(50,50)),(50,50,1))))#initially we obtained a 2d array. In order to fit it in our keras CNN model, we had to convert it into a 3d model. Also the image is downsized to avoid OOM (Out of Memory) exception\n",
    "        y.append(one_hot(val,size))#store our results by converting it into a one-hot vector. Here the enumerate () returning counter val palys a very important role."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Splitting the dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The data is splitted into training and cross validation sets. Cross validation size 15% of total data. It aids our results and gives a genuine boost\n",
    "in our accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "X_train,X_val,y_train,y_val=train_test_split(X,y,test_size=0.15, random_state=42)\n",
    "X_train_arr=np.array(X_train)\n",
    "X_val_arr=np.array(X_val)\n",
    "y_train_arr=np.array(y_train)\n",
    "y_val_arr=np.array(y_val)\n",
    "_,h,w,d=X_train_arr.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(62, (5, 5), activation=\"relu\", input_shape=(50, 50, 1..., padding=\"same\")`\n",
      "  \n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(124, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:8: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(62, (5, 5), activation=\"relu\", padding=\"same\")`\n",
      "  \n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_5 (Conv2D)            (None, 50, 50, 62)        1612      \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 25, 25, 62)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_6 (Conv2D)            (None, 25, 25, 124)       192324    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 12, 12, 124)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_7 (Conv2D)            (None, 12, 12, 62)        192262    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 6, 6, 62)          0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 6, 6, 62)          0         \n",
      "_________________________________________________________________\n",
      "flatten_2 (Flatten)          (None, 2232)              0         \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 62)                138446    \n",
      "=================================================================\n",
      "Total params: 524,644\n",
      "Trainable params: 524,644\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    " model=Sequential()\n",
    "model.add(Conv2D(62, (5, 5), activation='relu',border_mode='same',input_shape=(h,w,d)))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Conv2D(124, (5, 5), activation='relu',border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "'''model.add(Convolution2D(124, (5, 5), activation='relu',border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))'''\n",
    "model.add(Conv2D(62, (5, 5), activation='relu',border_mode='same'))\n",
    "model.add(MaxPooling2D(pool_size=(2,2)))\n",
    "model.add(Dropout(0.2))\n",
    "model.add(Flatten())\n",
    "'''model.add(Dense(1024, activation='relu'))\n",
    "model.add(Dropout(0.5))'''\n",
    "model.add(Dense(62, activation='softmax'))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2108 samples, validate on 372 samples\n",
      "Epoch 1/10\n",
      "2108/2108 [==============================] - ETA: 24:45 - loss: 4.1396 - acc: 0.0000e+ - ETA: 12:12 - loss: 4.1324 - acc: 0.0312   - ETA: 5:56 - loss: 4.1723 - acc: 0.0156 - ETA: 3:50 - loss: 4.1677 - acc: 0.015 - ETA: 2:47 - loss: 4.1588 - acc: 0.015 - ETA: 2:09 - loss: 4.1534 - acc: 0.015 - ETA: 1:44 - loss: 4.1500 - acc: 0.013 - ETA: 1:26 - loss: 4.1479 - acc: 0.011 - ETA: 1:12 - loss: 4.1457 - acc: 0.011 - ETA: 1:02 - loss: 4.1431 - acc: 0.019 - ETA: 53s - loss: 4.1414 - acc: 0.017 - ETA: 47s - loss: 4.1403 - acc: 0.01 - ETA: 41s - loss: 4.1394 - acc: 0.01 - ETA: 36s - loss: 4.1391 - acc: 0.01 - ETA: 32s - loss: 4.1388 - acc: 0.01 - ETA: 28s - loss: 4.1378 - acc: 0.01 - ETA: 25s - loss: 4.1370 - acc: 0.01 - ETA: 22s - loss: 4.1364 - acc: 0.01 - ETA: 19s - loss: 4.1358 - acc: 0.01 - ETA: 17s - loss: 4.1354 - acc: 0.01 - ETA: 15s - loss: 4.1348 - acc: 0.01 - ETA: 13s - loss: 4.1344 - acc: 0.01 - ETA: 12s - loss: 4.1339 - acc: 0.01 - ETA: 10s - loss: 4.1336 - acc: 0.01 - ETA: 9s - loss: 4.1334 - acc: 0.0176 - ETA: 7s - loss: 4.1328 - acc: 0.017 - ETA: 6s - loss: 4.1325 - acc: 0.018 - ETA: 5s - loss: 4.1317 - acc: 0.017 - ETA: 4s - loss: 4.1312 - acc: 0.017 - ETA: 3s - loss: 4.1300 - acc: 0.017 - ETA: 2s - loss: 4.1280 - acc: 0.017 - ETA: 1s - loss: 4.1289 - acc: 0.016 - ETA: 1s - loss: 4.1269 - acc: 0.017 - ETA: 0s - loss: 4.1261 - acc: 0.017 - ETA: 0s - loss: 4.1248 - acc: 0.017 - 27s 13ms/step - loss: 4.1239 - acc: 0.0176 - val_loss: 4.0533 - val_acc: 0.0269\n",
      "Epoch 2/10\n",
      "2108/2108 [==============================] - ETA: 4s - loss: 3.8814 - acc: 0.187 - ETA: 4s - loss: 3.9148 - acc: 0.125 - ETA: 4s - loss: 3.9101 - acc: 0.114 - ETA: 4s - loss: 3.8850 - acc: 0.101 - ETA: 4s - loss: 3.8949 - acc: 0.087 - ETA: 4s - loss: 3.9058 - acc: 0.078 - ETA: 4s - loss: 3.8875 - acc: 0.067 - ETA: 4s - loss: 3.8971 - acc: 0.066 - ETA: 4s - loss: 3.8792 - acc: 0.069 - ETA: 4s - loss: 3.8983 - acc: 0.062 - ETA: 4s - loss: 3.8857 - acc: 0.059 - ETA: 4s - loss: 3.8782 - acc: 0.065 - ETA: 4s - loss: 3.8832 - acc: 0.062 - ETA: 3s - loss: 3.8797 - acc: 0.064 - ETA: 3s - loss: 3.8557 - acc: 0.064 - ETA: 3s - loss: 3.8555 - acc: 0.064 - ETA: 3s - loss: 3.8523 - acc: 0.068 - ETA: 3s - loss: 3.8410 - acc: 0.071 - ETA: 3s - loss: 3.8205 - acc: 0.078 - ETA: 3s - loss: 3.8099 - acc: 0.081 - ETA: 3s - loss: 3.8098 - acc: 0.077 - ETA: 3s - loss: 3.8085 - acc: 0.079 - ETA: 3s - loss: 3.7848 - acc: 0.084 - ETA: 2s - loss: 3.7638 - acc: 0.086 - ETA: 2s - loss: 3.7351 - acc: 0.090 - ETA: 2s - loss: 3.7104 - acc: 0.090 - ETA: 2s - loss: 3.6846 - acc: 0.094 - ETA: 1s - loss: 3.6659 - acc: 0.101 - ETA: 1s - loss: 3.6402 - acc: 0.105 - ETA: 1s - loss: 3.6039 - acc: 0.110 - ETA: 1s - loss: 3.5859 - acc: 0.112 - ETA: 1s - loss: 3.5398 - acc: 0.122 - ETA: 1s - loss: 3.5047 - acc: 0.124 - ETA: 1s - loss: 3.4740 - acc: 0.133 - ETA: 0s - loss: 3.4441 - acc: 0.141 - ETA: 0s - loss: 3.4126 - acc: 0.146 - ETA: 0s - loss: 3.3811 - acc: 0.152 - ETA: 0s - loss: 3.3615 - acc: 0.156 - ETA: 0s - loss: 3.3482 - acc: 0.159 - ETA: 0s - loss: 3.3298 - acc: 0.164 - ETA: 0s - loss: 3.3127 - acc: 0.167 - ETA: 0s - loss: 3.2864 - acc: 0.173 - ETA: 0s - loss: 3.2687 - acc: 0.177 - ETA: 0s - loss: 3.2597 - acc: 0.179 - ETA: 0s - loss: 3.2440 - acc: 0.181 - ETA: 0s - loss: 3.2325 - acc: 0.183 - ETA: 0s - loss: 3.2178 - acc: 0.186 - ETA: 0s - loss: 3.2089 - acc: 0.189 - ETA: 0s - loss: 3.2017 - acc: 0.189 - ETA: 0s - loss: 3.1863 - acc: 0.192 - 4s 2ms/step - loss: 3.1724 - acc: 0.1959 - val_loss: 2.2700 - val_acc: 0.4167\n",
      "Epoch 3/10\n",
      "2108/2108 [==============================] - ETA: 4s - loss: 1.7677 - acc: 0.531 - ETA: 4s - loss: 1.8420 - acc: 0.468 - ETA: 4s - loss: 2.0238 - acc: 0.437 - ETA: 4s - loss: 2.2730 - acc: 0.390 - ETA: 4s - loss: 2.2446 - acc: 0.387 - ETA: 4s - loss: 2.2815 - acc: 0.375 - ETA: 4s - loss: 2.2233 - acc: 0.392 - ETA: 4s - loss: 2.1529 - acc: 0.425 - ETA: 4s - loss: 2.1275 - acc: 0.423 - ETA: 4s - loss: 2.1538 - acc: 0.421 - ETA: 4s - loss: 2.1314 - acc: 0.420 - ETA: 4s - loss: 2.0972 - acc: 0.429 - ETA: 4s - loss: 2.0706 - acc: 0.442 - ETA: 4s - loss: 2.0555 - acc: 0.448 - ETA: 3s - loss: 2.0577 - acc: 0.443 - ETA: 3s - loss: 2.0524 - acc: 0.451 - ETA: 3s - loss: 2.0685 - acc: 0.446 - ETA: 3s - loss: 2.0218 - acc: 0.463 - ETA: 3s - loss: 1.9803 - acc: 0.473 - ETA: 3s - loss: 1.9539 - acc: 0.475 - ETA: 3s - loss: 1.9408 - acc: 0.479 - ETA: 3s - loss: 1.9191 - acc: 0.481 - ETA: 3s - loss: 1.9069 - acc: 0.481 - ETA: 3s - loss: 1.8807 - acc: 0.484 - ETA: 3s - loss: 1.8734 - acc: 0.486 - ETA: 3s - loss: 1.8788 - acc: 0.482 - ETA: 3s - loss: 1.8661 - acc: 0.485 - ETA: 3s - loss: 1.8408 - acc: 0.488 - ETA: 2s - loss: 1.8351 - acc: 0.490 - ETA: 2s - loss: 1.8126 - acc: 0.496 - ETA: 2s - loss: 1.8065 - acc: 0.500 - ETA: 2s - loss: 1.8047 - acc: 0.502 - ETA: 2s - loss: 1.8088 - acc: 0.500 - ETA: 2s - loss: 1.8031 - acc: 0.508 - ETA: 2s - loss: 1.8051 - acc: 0.507 - ETA: 1s - loss: 1.7974 - acc: 0.504 - ETA: 1s - loss: 1.8040 - acc: 0.503 - ETA: 1s - loss: 1.7921 - acc: 0.508 - ETA: 1s - loss: 1.7709 - acc: 0.513 - ETA: 1s - loss: 1.7684 - acc: 0.514 - ETA: 1s - loss: 1.7564 - acc: 0.518 - ETA: 1s - loss: 1.7487 - acc: 0.521 - ETA: 1s - loss: 1.7471 - acc: 0.521 - ETA: 1s - loss: 1.7370 - acc: 0.522 - ETA: 1s - loss: 1.7265 - acc: 0.524 - ETA: 0s - loss: 1.7221 - acc: 0.525 - ETA: 0s - loss: 1.7238 - acc: 0.524 - ETA: 0s - loss: 1.7230 - acc: 0.523 - ETA: 0s - loss: 1.7189 - acc: 0.524 - ETA: 0s - loss: 1.7205 - acc: 0.522 - ETA: 0s - loss: 1.7192 - acc: 0.523 - ETA: 0s - loss: 1.7052 - acc: 0.526 - ETA: 0s - loss: 1.7012 - acc: 0.527 - ETA: 0s - loss: 1.6987 - acc: 0.528 - ETA: 0s - loss: 1.6979 - acc: 0.529 - ETA: 0s - loss: 1.6951 - acc: 0.530 - ETA: 0s - loss: 1.6886 - acc: 0.532 - ETA: 0s - loss: 1.6904 - acc: 0.533 - ETA: 0s - loss: 1.6804 - acc: 0.537 - 5s 2ms/step - loss: 1.6720 - acc: 0.5394 - val_loss: 1.5367 - val_acc: 0.5887\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 4s - loss: 0.8833 - acc: 0.843 - ETA: 4s - loss: 0.8431 - acc: 0.843 - ETA: 4s - loss: 0.9786 - acc: 0.781 - ETA: 4s - loss: 1.0266 - acc: 0.750 - ETA: 4s - loss: 1.0451 - acc: 0.725 - ETA: 4s - loss: 1.0036 - acc: 0.718 - ETA: 4s - loss: 1.0315 - acc: 0.700 - ETA: 4s - loss: 1.0140 - acc: 0.714 - ETA: 4s - loss: 1.0010 - acc: 0.718 - ETA: 4s - loss: 0.9840 - acc: 0.718 - ETA: 4s - loss: 0.9423 - acc: 0.730 - ETA: 4s - loss: 0.9906 - acc: 0.716 - ETA: 4s - loss: 0.9894 - acc: 0.709 - ETA: 4s - loss: 1.0098 - acc: 0.707 - ETA: 4s - loss: 1.0093 - acc: 0.702 - ETA: 4s - loss: 0.9747 - acc: 0.714 - ETA: 3s - loss: 0.9674 - acc: 0.716 - ETA: 3s - loss: 0.9710 - acc: 0.718 - ETA: 3s - loss: 0.9720 - acc: 0.715 - ETA: 3s - loss: 0.9724 - acc: 0.709 - ETA: 3s - loss: 0.9793 - acc: 0.707 - ETA: 3s - loss: 0.9855 - acc: 0.709 - ETA: 3s - loss: 1.0130 - acc: 0.703 - ETA: 3s - loss: 1.0152 - acc: 0.704 - ETA: 3s - loss: 1.0122 - acc: 0.703 - ETA: 2s - loss: 1.0240 - acc: 0.698 - ETA: 2s - loss: 1.0228 - acc: 0.698 - ETA: 2s - loss: 1.0194 - acc: 0.695 - ETA: 2s - loss: 1.0272 - acc: 0.693 - ETA: 2s - loss: 1.0358 - acc: 0.692 - ETA: 2s - loss: 1.0297 - acc: 0.693 - ETA: 2s - loss: 1.0251 - acc: 0.693 - ETA: 2s - loss: 1.0302 - acc: 0.689 - ETA: 2s - loss: 1.0358 - acc: 0.685 - ETA: 2s - loss: 1.0402 - acc: 0.684 - ETA: 2s - loss: 1.0440 - acc: 0.682 - ETA: 2s - loss: 1.0392 - acc: 0.684 - ETA: 2s - loss: 1.0392 - acc: 0.683 - ETA: 1s - loss: 1.0384 - acc: 0.683 - ETA: 1s - loss: 1.0453 - acc: 0.680 - ETA: 1s - loss: 1.0485 - acc: 0.678 - ETA: 1s - loss: 1.0620 - acc: 0.676 - ETA: 1s - loss: 1.0707 - acc: 0.674 - ETA: 1s - loss: 1.0812 - acc: 0.671 - ETA: 1s - loss: 1.0858 - acc: 0.670 - ETA: 1s - loss: 1.0782 - acc: 0.671 - ETA: 1s - loss: 1.0805 - acc: 0.672 - ETA: 1s - loss: 1.0807 - acc: 0.673 - ETA: 1s - loss: 1.0916 - acc: 0.672 - ETA: 1s - loss: 1.0973 - acc: 0.668 - ETA: 1s - loss: 1.1007 - acc: 0.666 - ETA: 0s - loss: 1.0993 - acc: 0.667 - ETA: 0s - loss: 1.1029 - acc: 0.664 - ETA: 0s - loss: 1.1004 - acc: 0.665 - ETA: 0s - loss: 1.0959 - acc: 0.666 - ETA: 0s - loss: 1.0907 - acc: 0.667 - ETA: 0s - loss: 1.0899 - acc: 0.666 - ETA: 0s - loss: 1.0916 - acc: 0.666 - ETA: 0s - loss: 1.0870 - acc: 0.668 - ETA: 0s - loss: 1.0936 - acc: 0.668 - ETA: 0s - loss: 1.0884 - acc: 0.670 - ETA: 0s - loss: 1.0889 - acc: 0.669 - ETA: 0s - loss: 1.0858 - acc: 0.670 - 5s 3ms/step - loss: 1.0834 - acc: 0.6698 - val_loss: 1.3646 - val_acc: 0.6398\n",
      "Epoch 5/10\n",
      "2108/2108 [==============================] - ETA: 2s - loss: 0.9327 - acc: 0.718 - ETA: 2s - loss: 0.9435 - acc: 0.739 - ETA: 2s - loss: 0.7616 - acc: 0.781 - ETA: 2s - loss: 0.6872 - acc: 0.799 - ETA: 2s - loss: 0.6987 - acc: 0.791 - ETA: 1s - loss: 0.7436 - acc: 0.784 - ETA: 2s - loss: 0.7049 - acc: 0.794 - ETA: 2s - loss: 0.7037 - acc: 0.790 - ETA: 2s - loss: 0.7167 - acc: 0.785 - ETA: 2s - loss: 0.7089 - acc: 0.791 - ETA: 2s - loss: 0.7157 - acc: 0.789 - ETA: 2s - loss: 0.7171 - acc: 0.783 - ETA: 2s - loss: 0.7179 - acc: 0.781 - ETA: 2s - loss: 0.7238 - acc: 0.781 - ETA: 2s - loss: 0.7354 - acc: 0.770 - ETA: 2s - loss: 0.7337 - acc: 0.772 - ETA: 2s - loss: 0.7558 - acc: 0.768 - ETA: 2s - loss: 0.7531 - acc: 0.764 - ETA: 2s - loss: 0.7409 - acc: 0.769 - ETA: 2s - loss: 0.7451 - acc: 0.767 - ETA: 2s - loss: 0.7495 - acc: 0.766 - ETA: 2s - loss: 0.7518 - acc: 0.768 - ETA: 2s - loss: 0.7495 - acc: 0.771 - ETA: 2s - loss: 0.7566 - acc: 0.770 - ETA: 2s - loss: 0.7679 - acc: 0.768 - ETA: 2s - loss: 0.7565 - acc: 0.773 - ETA: 2s - loss: 0.7629 - acc: 0.771 - ETA: 2s - loss: 0.7579 - acc: 0.773 - ETA: 2s - loss: 0.7582 - acc: 0.774 - ETA: 2s - loss: 0.7571 - acc: 0.775 - ETA: 2s - loss: 0.7516 - acc: 0.777 - ETA: 1s - loss: 0.7454 - acc: 0.779 - ETA: 1s - loss: 0.7457 - acc: 0.778 - ETA: 1s - loss: 0.7407 - acc: 0.780 - ETA: 1s - loss: 0.7400 - acc: 0.781 - ETA: 1s - loss: 0.7470 - acc: 0.777 - ETA: 1s - loss: 0.7443 - acc: 0.777 - ETA: 1s - loss: 0.7396 - acc: 0.779 - ETA: 1s - loss: 0.7431 - acc: 0.777 - ETA: 1s - loss: 0.7380 - acc: 0.779 - ETA: 1s - loss: 0.7307 - acc: 0.781 - ETA: 1s - loss: 0.7287 - acc: 0.782 - ETA: 1s - loss: 0.7236 - acc: 0.783 - ETA: 1s - loss: 0.7239 - acc: 0.782 - ETA: 1s - loss: 0.7345 - acc: 0.779 - ETA: 1s - loss: 0.7368 - acc: 0.778 - ETA: 1s - loss: 0.7334 - acc: 0.778 - ETA: 0s - loss: 0.7345 - acc: 0.779 - ETA: 0s - loss: 0.7393 - acc: 0.778 - ETA: 0s - loss: 0.7463 - acc: 0.777 - ETA: 0s - loss: 0.7455 - acc: 0.777 - ETA: 0s - loss: 0.7455 - acc: 0.778 - ETA: 0s - loss: 0.7460 - acc: 0.776 - ETA: 0s - loss: 0.7464 - acc: 0.776 - ETA: 0s - loss: 0.7398 - acc: 0.778 - 4s 2ms/step - loss: 0.7345 - acc: 0.7804 - val_loss: 1.2472 - val_acc: 0.6640\n",
      "Epoch 6/10\n",
      "2108/2108 [==============================] - ETA: 1s - loss: 0.6064 - acc: 0.843 - ETA: 6s - loss: 0.6218 - acc: 0.812 - ETA: 5s - loss: 0.6112 - acc: 0.812 - ETA: 5s - loss: 0.5337 - acc: 0.843 - ETA: 5s - loss: 0.5102 - acc: 0.843 - ETA: 5s - loss: 0.5428 - acc: 0.828 - ETA: 5s - loss: 0.5308 - acc: 0.825 - ETA: 4s - loss: 0.5257 - acc: 0.832 - ETA: 4s - loss: 0.5741 - acc: 0.826 - ETA: 4s - loss: 0.5994 - acc: 0.821 - ETA: 4s - loss: 0.5858 - acc: 0.821 - ETA: 4s - loss: 0.5664 - acc: 0.828 - ETA: 4s - loss: 0.5646 - acc: 0.829 - ETA: 4s - loss: 0.5675 - acc: 0.828 - ETA: 4s - loss: 0.5567 - acc: 0.833 - ETA: 4s - loss: 0.5443 - acc: 0.839 - ETA: 4s - loss: 0.5420 - acc: 0.836 - ETA: 3s - loss: 0.5630 - acc: 0.831 - ETA: 3s - loss: 0.5528 - acc: 0.833 - ETA: 3s - loss: 0.5515 - acc: 0.831 - ETA: 3s - loss: 0.5478 - acc: 0.834 - ETA: 3s - loss: 0.5415 - acc: 0.836 - ETA: 3s - loss: 0.5349 - acc: 0.839 - ETA: 3s - loss: 0.5429 - acc: 0.839 - ETA: 3s - loss: 0.5447 - acc: 0.837 - ETA: 3s - loss: 0.5386 - acc: 0.837 - ETA: 3s - loss: 0.5470 - acc: 0.832 - ETA: 3s - loss: 0.5456 - acc: 0.831 - ETA: 2s - loss: 0.5357 - acc: 0.836 - ETA: 2s - loss: 0.5418 - acc: 0.832 - ETA: 2s - loss: 0.5434 - acc: 0.829 - ETA: 2s - loss: 0.5490 - acc: 0.830 - ETA: 2s - loss: 0.5528 - acc: 0.826 - ETA: 2s - loss: 0.5534 - acc: 0.827 - ETA: 2s - loss: 0.5546 - acc: 0.827 - ETA: 2s - loss: 0.5548 - acc: 0.827 - ETA: 2s - loss: 0.5524 - acc: 0.826 - ETA: 2s - loss: 0.5574 - acc: 0.824 - ETA: 2s - loss: 0.5635 - acc: 0.822 - ETA: 2s - loss: 0.5681 - acc: 0.821 - ETA: 2s - loss: 0.5755 - acc: 0.818 - ETA: 1s - loss: 0.5766 - acc: 0.818 - ETA: 1s - loss: 0.5797 - acc: 0.817 - ETA: 1s - loss: 0.5838 - acc: 0.812 - ETA: 1s - loss: 0.5885 - acc: 0.813 - ETA: 1s - loss: 0.5949 - acc: 0.811 - ETA: 1s - loss: 0.6017 - acc: 0.809 - ETA: 0s - loss: 0.6026 - acc: 0.808 - ETA: 0s - loss: 0.5966 - acc: 0.810 - ETA: 0s - loss: 0.5989 - acc: 0.809 - ETA: 0s - loss: 0.5942 - acc: 0.810 - ETA: 0s - loss: 0.5942 - acc: 0.811 - ETA: 0s - loss: 0.5932 - acc: 0.812 - ETA: 0s - loss: 0.5915 - acc: 0.812 - ETA: 0s - loss: 0.5936 - acc: 0.811 - ETA: 0s - loss: 0.5895 - acc: 0.812 - 5s 2ms/step - loss: 0.5892 - acc: 0.8131 - val_loss: 1.1623 - val_acc: 0.6882\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 4s - loss: 0.5513 - acc: 0.812 - ETA: 4s - loss: 0.4516 - acc: 0.843 - ETA: 4s - loss: 0.4134 - acc: 0.854 - ETA: 4s - loss: 0.4001 - acc: 0.859 - ETA: 4s - loss: 0.4041 - acc: 0.862 - ETA: 4s - loss: 0.3825 - acc: 0.869 - ETA: 4s - loss: 0.4197 - acc: 0.843 - ETA: 4s - loss: 0.4086 - acc: 0.851 - ETA: 4s - loss: 0.4014 - acc: 0.854 - ETA: 4s - loss: 0.3939 - acc: 0.856 - ETA: 4s - loss: 0.3786 - acc: 0.866 - ETA: 4s - loss: 0.3759 - acc: 0.867 - ETA: 4s - loss: 0.3624 - acc: 0.875 - ETA: 4s - loss: 0.3695 - acc: 0.870 - ETA: 4s - loss: 0.3722 - acc: 0.870 - ETA: 3s - loss: 0.3958 - acc: 0.863 - ETA: 3s - loss: 0.4157 - acc: 0.860 - ETA: 3s - loss: 0.4099 - acc: 0.866 - ETA: 3s - loss: 0.4185 - acc: 0.865 - ETA: 3s - loss: 0.4187 - acc: 0.865 - ETA: 3s - loss: 0.4063 - acc: 0.870 - ETA: 3s - loss: 0.4091 - acc: 0.869 - ETA: 3s - loss: 0.4097 - acc: 0.869 - ETA: 3s - loss: 0.4174 - acc: 0.868 - ETA: 3s - loss: 0.4118 - acc: 0.871 - ETA: 3s - loss: 0.4051 - acc: 0.875 - ETA: 3s - loss: 0.4065 - acc: 0.876 - ETA: 2s - loss: 0.4164 - acc: 0.872 - ETA: 2s - loss: 0.4211 - acc: 0.869 - ETA: 2s - loss: 0.4116 - acc: 0.872 - ETA: 2s - loss: 0.4194 - acc: 0.872 - ETA: 2s - loss: 0.4247 - acc: 0.867 - ETA: 2s - loss: 0.4281 - acc: 0.866 - ETA: 2s - loss: 0.4378 - acc: 0.859 - ETA: 1s - loss: 0.4268 - acc: 0.862 - ETA: 1s - loss: 0.4232 - acc: 0.862 - ETA: 1s - loss: 0.4174 - acc: 0.863 - ETA: 1s - loss: 0.4152 - acc: 0.862 - ETA: 1s - loss: 0.4131 - acc: 0.861 - ETA: 1s - loss: 0.4147 - acc: 0.861 - ETA: 0s - loss: 0.4104 - acc: 0.864 - ETA: 0s - loss: 0.4155 - acc: 0.862 - ETA: 0s - loss: 0.4169 - acc: 0.862 - ETA: 0s - loss: 0.4197 - acc: 0.862 - ETA: 0s - loss: 0.4190 - acc: 0.863 - ETA: 0s - loss: 0.4183 - acc: 0.862 - ETA: 0s - loss: 0.4190 - acc: 0.861 - ETA: 0s - loss: 0.4261 - acc: 0.859 - ETA: 0s - loss: 0.4263 - acc: 0.859 - ETA: 0s - loss: 0.4253 - acc: 0.858 - ETA: 0s - loss: 0.4237 - acc: 0.859 - ETA: 0s - loss: 0.4250 - acc: 0.858 - ETA: 0s - loss: 0.4253 - acc: 0.858 - ETA: 0s - loss: 0.4227 - acc: 0.859 - ETA: 0s - loss: 0.4220 - acc: 0.858 - 5s 2ms/step - loss: 0.4188 - acc: 0.8596 - val_loss: 1.2926 - val_acc: 0.6962\n",
      "Epoch 8/10\n",
      "2108/2108 [==============================] - ETA: 4s - loss: 0.4898 - acc: 0.843 - ETA: 4s - loss: 0.4193 - acc: 0.875 - ETA: 4s - loss: 0.3196 - acc: 0.906 - ETA: 4s - loss: 0.3039 - acc: 0.898 - ETA: 4s - loss: 0.3536 - acc: 0.881 - ETA: 4s - loss: 0.3520 - acc: 0.875 - ETA: 4s - loss: 0.3576 - acc: 0.861 - ETA: 4s - loss: 0.3665 - acc: 0.863 - ETA: 4s - loss: 0.3631 - acc: 0.864 - ETA: 4s - loss: 0.3470 - acc: 0.875 - ETA: 4s - loss: 0.3392 - acc: 0.875 - ETA: 4s - loss: 0.3591 - acc: 0.869 - ETA: 4s - loss: 0.3517 - acc: 0.872 - ETA: 4s - loss: 0.3370 - acc: 0.879 - ETA: 4s - loss: 0.3274 - acc: 0.883 - ETA: 3s - loss: 0.3320 - acc: 0.884 - ETA: 3s - loss: 0.3303 - acc: 0.887 - ETA: 3s - loss: 0.3216 - acc: 0.894 - ETA: 3s - loss: 0.3212 - acc: 0.894 - ETA: 3s - loss: 0.3238 - acc: 0.892 - ETA: 3s - loss: 0.3286 - acc: 0.889 - ETA: 3s - loss: 0.3186 - acc: 0.892 - ETA: 3s - loss: 0.3186 - acc: 0.891 - ETA: 2s - loss: 0.3279 - acc: 0.886 - ETA: 2s - loss: 0.3155 - acc: 0.890 - ETA: 2s - loss: 0.3191 - acc: 0.889 - ETA: 2s - loss: 0.3177 - acc: 0.892 - ETA: 2s - loss: 0.3222 - acc: 0.892 - ETA: 1s - loss: 0.3253 - acc: 0.889 - ETA: 1s - loss: 0.3264 - acc: 0.888 - ETA: 1s - loss: 0.3253 - acc: 0.890 - ETA: 1s - loss: 0.3204 - acc: 0.891 - ETA: 1s - loss: 0.3251 - acc: 0.889 - ETA: 1s - loss: 0.3218 - acc: 0.891 - ETA: 1s - loss: 0.3217 - acc: 0.892 - ETA: 1s - loss: 0.3215 - acc: 0.893 - ETA: 1s - loss: 0.3289 - acc: 0.892 - ETA: 1s - loss: 0.3319 - acc: 0.890 - ETA: 1s - loss: 0.3303 - acc: 0.891 - ETA: 0s - loss: 0.3283 - acc: 0.892 - ETA: 0s - loss: 0.3296 - acc: 0.891 - ETA: 0s - loss: 0.3289 - acc: 0.892 - ETA: 0s - loss: 0.3275 - acc: 0.893 - ETA: 0s - loss: 0.3278 - acc: 0.893 - ETA: 0s - loss: 0.3297 - acc: 0.891 - ETA: 0s - loss: 0.3330 - acc: 0.890 - ETA: 0s - loss: 0.3299 - acc: 0.891 - ETA: 0s - loss: 0.3369 - acc: 0.889 - ETA: 0s - loss: 0.3383 - acc: 0.889 - ETA: 0s - loss: 0.3381 - acc: 0.889 - ETA: 0s - loss: 0.3404 - acc: 0.889 - ETA: 0s - loss: 0.3386 - acc: 0.890 - ETA: 0s - loss: 0.3380 - acc: 0.891 - ETA: 0s - loss: 0.3356 - acc: 0.891 - 5s 2ms/step - loss: 0.3329 - acc: 0.8928 - val_loss: 1.2206 - val_acc: 0.7016\n",
      "Epoch 9/10\n",
      "2108/2108 [==============================] - ETA: 4s - loss: 0.1648 - acc: 0.937 - ETA: 4s - loss: 0.2367 - acc: 0.875 - ETA: 4s - loss: 0.1929 - acc: 0.906 - ETA: 4s - loss: 0.2046 - acc: 0.914 - ETA: 4s - loss: 0.2456 - acc: 0.906 - ETA: 4s - loss: 0.2487 - acc: 0.895 - ETA: 4s - loss: 0.2483 - acc: 0.897 - ETA: 4s - loss: 0.2404 - acc: 0.902 - ETA: 4s - loss: 0.2580 - acc: 0.902 - ETA: 4s - loss: 0.2591 - acc: 0.903 - ETA: 4s - loss: 0.2596 - acc: 0.906 - ETA: 4s - loss: 0.2546 - acc: 0.911 - ETA: 4s - loss: 0.2605 - acc: 0.917 - ETA: 3s - loss: 0.2548 - acc: 0.921 - ETA: 3s - loss: 0.2693 - acc: 0.918 - ETA: 2s - loss: 0.2655 - acc: 0.920 - ETA: 2s - loss: 0.2711 - acc: 0.919 - ETA: 2s - loss: 0.2835 - acc: 0.915 - ETA: 2s - loss: 0.2953 - acc: 0.909 - ETA: 2s - loss: 0.2883 - acc: 0.911 - ETA: 1s - loss: 0.2889 - acc: 0.908 - ETA: 1s - loss: 0.2941 - acc: 0.904 - ETA: 1s - loss: 0.2845 - acc: 0.908 - ETA: 1s - loss: 0.2804 - acc: 0.908 - ETA: 1s - loss: 0.2801 - acc: 0.910 - ETA: 1s - loss: 0.2827 - acc: 0.909 - ETA: 1s - loss: 0.2862 - acc: 0.906 - ETA: 1s - loss: 0.2847 - acc: 0.907 - ETA: 0s - loss: 0.2813 - acc: 0.906 - ETA: 0s - loss: 0.2775 - acc: 0.908 - ETA: 0s - loss: 0.2730 - acc: 0.910 - ETA: 0s - loss: 0.2687 - acc: 0.912 - ETA: 0s - loss: 0.2680 - acc: 0.913 - ETA: 0s - loss: 0.2671 - acc: 0.913 - ETA: 0s - loss: 0.2643 - acc: 0.915 - ETA: 0s - loss: 0.2662 - acc: 0.914 - ETA: 0s - loss: 0.2690 - acc: 0.914 - ETA: 0s - loss: 0.2712 - acc: 0.915 - ETA: 0s - loss: 0.2706 - acc: 0.915 - ETA: 0s - loss: 0.2690 - acc: 0.914 - ETA: 0s - loss: 0.2660 - acc: 0.916 - ETA: 0s - loss: 0.2684 - acc: 0.915 - ETA: 0s - loss: 0.2684 - acc: 0.915 - ETA: 0s - loss: 0.2734 - acc: 0.913 - ETA: 0s - loss: 0.2763 - acc: 0.912 - ETA: 0s - loss: 0.2729 - acc: 0.913 - ETA: 0s - loss: 0.2720 - acc: 0.913 - 4s 2ms/step - loss: 0.2690 - acc: 0.9151 - val_loss: 1.3846 - val_acc: 0.6882\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 4s - loss: 0.1230 - acc: 0.968 - ETA: 4s - loss: 0.2458 - acc: 0.921 - ETA: 4s - loss: 0.2419 - acc: 0.916 - ETA: 4s - loss: 0.2259 - acc: 0.921 - ETA: 4s - loss: 0.2624 - acc: 0.900 - ETA: 4s - loss: 0.2440 - acc: 0.911 - ETA: 4s - loss: 0.2287 - acc: 0.919 - ETA: 4s - loss: 0.2096 - acc: 0.929 - ETA: 4s - loss: 0.2095 - acc: 0.930 - ETA: 4s - loss: 0.2112 - acc: 0.928 - ETA: 4s - loss: 0.2109 - acc: 0.929 - ETA: 4s - loss: 0.2065 - acc: 0.932 - ETA: 4s - loss: 0.2110 - acc: 0.927 - ETA: 4s - loss: 0.2034 - acc: 0.930 - ETA: 3s - loss: 0.2141 - acc: 0.925 - ETA: 3s - loss: 0.2225 - acc: 0.923 - ETA: 4s - loss: 0.2208 - acc: 0.926 - ETA: 3s - loss: 0.2312 - acc: 0.927 - ETA: 3s - loss: 0.2308 - acc: 0.926 - ETA: 3s - loss: 0.2307 - acc: 0.926 - ETA: 3s - loss: 0.2255 - acc: 0.928 - ETA: 3s - loss: 0.2184 - acc: 0.931 - ETA: 3s - loss: 0.2208 - acc: 0.929 - ETA: 3s - loss: 0.2267 - acc: 0.927 - ETA: 2s - loss: 0.2229 - acc: 0.928 - ETA: 2s - loss: 0.2264 - acc: 0.928 - ETA: 2s - loss: 0.2240 - acc: 0.930 - ETA: 2s - loss: 0.2283 - acc: 0.929 - ETA: 2s - loss: 0.2367 - acc: 0.926 - ETA: 2s - loss: 0.2359 - acc: 0.925 - ETA: 2s - loss: 0.2348 - acc: 0.925 - ETA: 2s - loss: 0.2403 - acc: 0.923 - ETA: 1s - loss: 0.2410 - acc: 0.922 - ETA: 1s - loss: 0.2412 - acc: 0.922 - ETA: 1s - loss: 0.2386 - acc: 0.922 - ETA: 1s - loss: 0.2381 - acc: 0.923 - ETA: 1s - loss: 0.2383 - acc: 0.921 - ETA: 1s - loss: 0.2372 - acc: 0.921 - ETA: 1s - loss: 0.2350 - acc: 0.921 - ETA: 1s - loss: 0.2352 - acc: 0.922 - ETA: 1s - loss: 0.2330 - acc: 0.923 - ETA: 1s - loss: 0.2362 - acc: 0.922 - ETA: 1s - loss: 0.2333 - acc: 0.923 - ETA: 1s - loss: 0.2344 - acc: 0.922 - ETA: 1s - loss: 0.2329 - acc: 0.922 - ETA: 1s - loss: 0.2314 - acc: 0.923 - ETA: 1s - loss: 0.2307 - acc: 0.922 - ETA: 0s - loss: 0.2366 - acc: 0.921 - ETA: 0s - loss: 0.2360 - acc: 0.920 - ETA: 0s - loss: 0.2368 - acc: 0.920 - ETA: 0s - loss: 0.2362 - acc: 0.921 - ETA: 0s - loss: 0.2355 - acc: 0.921 - ETA: 0s - loss: 0.2352 - acc: 0.921 - ETA: 0s - loss: 0.2357 - acc: 0.922 - ETA: 0s - loss: 0.2335 - acc: 0.922 - ETA: 0s - loss: 0.2367 - acc: 0.921 - ETA: 0s - loss: 0.2351 - acc: 0.921 - ETA: 0s - loss: 0.2335 - acc: 0.922 - ETA: 0s - loss: 0.2336 - acc: 0.921 - ETA: 0s - loss: 0.2319 - acc: 0.922 - 5s 2ms/step - loss: 0.2308 - acc: 0.9227 - val_loss: 1.3270 - val_acc: 0.7097\n",
      "{'val_loss': [4.053336363966747, 2.269971292505982, 1.5366875901017139, 1.3645854707687133, 1.2471531527016753, 1.1623385593455324, 1.2926046425296414, 1.2206370852967745, 1.3846009841529272, 1.3269637451376965], 'val_acc': [0.026881720510221297, 0.416666667948487, 0.5887096774193549, 0.6397849462365591, 0.6639784959054762, 0.6881720423698425, 0.6962365597806951, 0.7016129045076268, 0.6881720436516628, 0.7096774180730184], 'loss': [4.123877104590921, 3.172393174958636, 1.671967854762213, 1.0833607029869616, 0.7345003727151954, 0.589180368286835, 0.41877479203738116, 0.33291257530507373, 0.26904279535788284, 0.23083408346551645], 'acc': [0.01755218218439444, 0.19592030343566041, 0.5393738137024403, 0.669829222124487, 0.7803605311961961, 0.8130929792402365, 0.8595825429207007, 0.8927893740402453, 0.9150853889943074, 0.9226755214823265]}\n"
     ]
    }
   ],
   "source": [
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "feeds=model.fit(x=X_train_arr,y=y_train_arr,epochs=10,validation_data=(X_val_arr,y_val_arr))\n",
    "print(feeds.history)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "The previous code gave good output but not very high accuracy hence we tried our model 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2108 samples, validate on 372 samples\n",
      "Epoch 1/10\n",
      "2108/2108 [==============================] - ETA: 2:54 - loss: 4.1289 - acc: 0.0000e+0 - ETA: 1:27 - loss: 4.1350 - acc: 0.0000e+0 - ETA: 58s - loss: 4.1529 - acc: 0.0104    - ETA: 44s - loss: 4.1314 - acc: 0.02 - ETA: 35s - loss: 4.1492 - acc: 0.01 - ETA: 29s - loss: 4.1529 - acc: 0.02 - ETA: 25s - loss: 4.1506 - acc: 0.01 - ETA: 22s - loss: 4.1477 - acc: 0.01 - ETA: 19s - loss: 4.1456 - acc: 0.01 - ETA: 16s - loss: 4.1434 - acc: 0.01 - ETA: 15s - loss: 4.1421 - acc: 0.01 - ETA: 14s - loss: 4.1408 - acc: 0.01 - ETA: 14s - loss: 4.1402 - acc: 0.01 - ETA: 13s - loss: 4.1395 - acc: 0.01 - ETA: 12s - loss: 4.1387 - acc: 0.01 - ETA: 12s - loss: 4.1389 - acc: 0.02 - ETA: 11s - loss: 4.1384 - acc: 0.02 - ETA: 11s - loss: 4.1377 - acc: 0.02 - ETA: 10s - loss: 4.1371 - acc: 0.02 - ETA: 10s - loss: 4.1364 - acc: 0.02 - ETA: 9s - loss: 4.1361 - acc: 0.0213 - ETA: 9s - loss: 4.1356 - acc: 0.020 - ETA: 9s - loss: 4.1352 - acc: 0.019 - ETA: 8s - loss: 4.1348 - acc: 0.018 - ETA: 8s - loss: 4.1343 - acc: 0.020 - ETA: 8s - loss: 4.1340 - acc: 0.019 - ETA: 7s - loss: 4.1342 - acc: 0.019 - ETA: 7s - loss: 4.1342 - acc: 0.019 - ETA: 7s - loss: 4.1341 - acc: 0.018 - ETA: 6s - loss: 4.1338 - acc: 0.018 - ETA: 6s - loss: 4.1336 - acc: 0.017 - ETA: 6s - loss: 4.1335 - acc: 0.017 - ETA: 6s - loss: 4.1334 - acc: 0.017 - ETA: 6s - loss: 4.1334 - acc: 0.017 - ETA: 5s - loss: 4.1331 - acc: 0.018 - ETA: 5s - loss: 4.1330 - acc: 0.018 - ETA: 5s - loss: 4.1328 - acc: 0.018 - ETA: 4s - loss: 4.1325 - acc: 0.018 - ETA: 4s - loss: 4.1324 - acc: 0.018 - ETA: 4s - loss: 4.1323 - acc: 0.017 - ETA: 4s - loss: 4.1323 - acc: 0.017 - ETA: 3s - loss: 4.1320 - acc: 0.017 - ETA: 3s - loss: 4.1317 - acc: 0.017 - ETA: 3s - loss: 4.1317 - acc: 0.017 - ETA: 3s - loss: 4.1316 - acc: 0.017 - ETA: 3s - loss: 4.1318 - acc: 0.016 - ETA: 2s - loss: 4.1315 - acc: 0.016 - ETA: 2s - loss: 4.1310 - acc: 0.017 - ETA: 2s - loss: 4.1306 - acc: 0.019 - ETA: 2s - loss: 4.1304 - acc: 0.019 - ETA: 2s - loss: 4.1303 - acc: 0.019 - ETA: 2s - loss: 4.1293 - acc: 0.020 - ETA: 1s - loss: 4.1286 - acc: 0.020 - ETA: 1s - loss: 4.1292 - acc: 0.019 - ETA: 1s - loss: 4.1298 - acc: 0.019 - ETA: 1s - loss: 4.1294 - acc: 0.019 - ETA: 1s - loss: 4.1294 - acc: 0.020 - ETA: 1s - loss: 4.1300 - acc: 0.020 - ETA: 0s - loss: 4.1295 - acc: 0.019 - ETA: 0s - loss: 4.1292 - acc: 0.019 - ETA: 0s - loss: 4.1289 - acc: 0.019 - ETA: 0s - loss: 4.1284 - acc: 0.020 - ETA: 0s - loss: 4.1279 - acc: 0.021 - ETA: 0s - loss: 4.1274 - acc: 0.020 - 12s 6ms/step - loss: 4.1269 - acc: 0.0204 - val_loss: 4.0703 - val_acc: 0.0242\n",
      "Epoch 2/10\n",
      "2108/2108 [==============================] - ETA: 3s - loss: 4.0566 - acc: 0.062 - ETA: 3s - loss: 4.0931 - acc: 0.046 - ETA: 3s - loss: 4.0759 - acc: 0.041 - ETA: 3s - loss: 4.0716 - acc: 0.031 - ETA: 3s - loss: 4.0560 - acc: 0.025 - ETA: 3s - loss: 4.0330 - acc: 0.026 - ETA: 3s - loss: 4.0286 - acc: 0.026 - ETA: 5s - loss: 4.0129 - acc: 0.035 - ETA: 5s - loss: 3.9928 - acc: 0.034 - ETA: 5s - loss: 3.9978 - acc: 0.034 - ETA: 5s - loss: 3.9934 - acc: 0.034 - ETA: 5s - loss: 3.9818 - acc: 0.033 - ETA: 5s - loss: 3.9702 - acc: 0.036 - ETA: 5s - loss: 3.9670 - acc: 0.040 - ETA: 5s - loss: 3.9566 - acc: 0.041 - ETA: 5s - loss: 3.9269 - acc: 0.044 - ETA: 5s - loss: 3.9187 - acc: 0.046 - ETA: 5s - loss: 3.9111 - acc: 0.048 - ETA: 5s - loss: 3.9082 - acc: 0.047 - ETA: 5s - loss: 3.8953 - acc: 0.050 - ETA: 5s - loss: 3.8990 - acc: 0.050 - ETA: 5s - loss: 3.8869 - acc: 0.049 - ETA: 5s - loss: 3.8679 - acc: 0.054 - ETA: 5s - loss: 3.8683 - acc: 0.054 - ETA: 5s - loss: 3.8571 - acc: 0.056 - ETA: 5s - loss: 3.8458 - acc: 0.056 - ETA: 4s - loss: 3.8389 - acc: 0.056 - ETA: 4s - loss: 3.8350 - acc: 0.059 - ETA: 4s - loss: 3.8165 - acc: 0.059 - ETA: 4s - loss: 3.7960 - acc: 0.063 - ETA: 4s - loss: 3.7800 - acc: 0.065 - ETA: 4s - loss: 3.7672 - acc: 0.069 - ETA: 4s - loss: 3.7501 - acc: 0.074 - ETA: 4s - loss: 3.7427 - acc: 0.079 - ETA: 3s - loss: 3.7361 - acc: 0.079 - ETA: 3s - loss: 3.7221 - acc: 0.083 - ETA: 3s - loss: 3.7079 - acc: 0.086 - ETA: 3s - loss: 3.6999 - acc: 0.087 - ETA: 3s - loss: 3.6893 - acc: 0.089 - ETA: 3s - loss: 3.6758 - acc: 0.092 - ETA: 2s - loss: 3.6600 - acc: 0.096 - ETA: 2s - loss: 3.6541 - acc: 0.096 - ETA: 2s - loss: 3.6451 - acc: 0.098 - ETA: 2s - loss: 3.6359 - acc: 0.097 - ETA: 2s - loss: 3.6248 - acc: 0.098 - ETA: 2s - loss: 3.6123 - acc: 0.100 - ETA: 2s - loss: 3.6075 - acc: 0.101 - ETA: 1s - loss: 3.6093 - acc: 0.101 - ETA: 1s - loss: 3.5938 - acc: 0.104 - ETA: 1s - loss: 3.5798 - acc: 0.106 - ETA: 1s - loss: 3.5764 - acc: 0.107 - ETA: 1s - loss: 3.5611 - acc: 0.111 - ETA: 1s - loss: 3.5494 - acc: 0.113 - ETA: 1s - loss: 3.5364 - acc: 0.115 - ETA: 1s - loss: 3.5227 - acc: 0.117 - ETA: 1s - loss: 3.5146 - acc: 0.118 - ETA: 1s - loss: 3.5010 - acc: 0.120 - ETA: 0s - loss: 3.4929 - acc: 0.120 - ETA: 0s - loss: 3.4890 - acc: 0.122 - ETA: 0s - loss: 3.4821 - acc: 0.124 - ETA: 0s - loss: 3.4633 - acc: 0.128 - ETA: 0s - loss: 3.4481 - acc: 0.130 - ETA: 0s - loss: 3.4271 - acc: 0.135 - ETA: 0s - loss: 3.4146 - acc: 0.138 - ETA: 0s - loss: 3.4000 - acc: 0.140 - 8s 4ms/step - loss: 3.3897 - acc: 0.1428 - val_loss: 2.7917 - val_acc: 0.2634\n",
      "Epoch 3/10\n",
      "2108/2108 [==============================] - ETA: 3s - loss: 2.3583 - acc: 0.375 - ETA: 3s - loss: 2.3301 - acc: 0.390 - ETA: 4s - loss: 2.3089 - acc: 0.395 - ETA: 3s - loss: 2.2814 - acc: 0.406 - ETA: 3s - loss: 2.2302 - acc: 0.425 - ETA: 3s - loss: 2.1793 - acc: 0.411 - ETA: 3s - loss: 2.1392 - acc: 0.419 - ETA: 3s - loss: 2.1002 - acc: 0.437 - ETA: 3s - loss: 2.1047 - acc: 0.444 - ETA: 3s - loss: 2.0645 - acc: 0.450 - ETA: 3s - loss: 2.0835 - acc: 0.446 - ETA: 3s - loss: 2.0628 - acc: 0.445 - ETA: 3s - loss: 2.0891 - acc: 0.442 - ETA: 3s - loss: 2.0879 - acc: 0.439 - ETA: 3s - loss: 2.0816 - acc: 0.439 - ETA: 3s - loss: 2.0600 - acc: 0.447 - ETA: 3s - loss: 2.0422 - acc: 0.450 - ETA: 3s - loss: 2.0547 - acc: 0.444 - ETA: 3s - loss: 2.0581 - acc: 0.445 - ETA: 3s - loss: 2.0744 - acc: 0.440 - ETA: 2s - loss: 2.0616 - acc: 0.443 - ETA: 2s - loss: 2.0537 - acc: 0.440 - ETA: 3s - loss: 2.0441 - acc: 0.444 - ETA: 3s - loss: 2.0603 - acc: 0.442 - ETA: 3s - loss: 2.0605 - acc: 0.440 - ETA: 3s - loss: 2.0601 - acc: 0.437 - ETA: 3s - loss: 2.0550 - acc: 0.437 - ETA: 3s - loss: 2.0544 - acc: 0.438 - ETA: 3s - loss: 2.0323 - acc: 0.447 - ETA: 3s - loss: 2.0344 - acc: 0.444 - ETA: 3s - loss: 2.0346 - acc: 0.445 - ETA: 3s - loss: 2.0412 - acc: 0.443 - ETA: 3s - loss: 2.0414 - acc: 0.445 - ETA: 3s - loss: 2.0479 - acc: 0.445 - ETA: 2s - loss: 2.0351 - acc: 0.449 - ETA: 2s - loss: 2.0418 - acc: 0.449 - ETA: 2s - loss: 2.0364 - acc: 0.450 - ETA: 2s - loss: 2.0273 - acc: 0.450 - ETA: 2s - loss: 2.0247 - acc: 0.447 - ETA: 2s - loss: 2.0128 - acc: 0.450 - ETA: 2s - loss: 2.0110 - acc: 0.450 - ETA: 2s - loss: 1.9995 - acc: 0.453 - ETA: 2s - loss: 1.9859 - acc: 0.454 - ETA: 2s - loss: 1.9824 - acc: 0.457 - ETA: 2s - loss: 1.9725 - acc: 0.460 - ETA: 2s - loss: 1.9681 - acc: 0.460 - ETA: 2s - loss: 1.9572 - acc: 0.462 - ETA: 1s - loss: 1.9486 - acc: 0.465 - ETA: 1s - loss: 1.9455 - acc: 0.466 - ETA: 1s - loss: 1.9411 - acc: 0.468 - ETA: 1s - loss: 1.9345 - acc: 0.468 - ETA: 1s - loss: 1.9453 - acc: 0.465 - ETA: 1s - loss: 1.9393 - acc: 0.467 - ETA: 1s - loss: 1.9337 - acc: 0.468 - ETA: 1s - loss: 1.9284 - acc: 0.469 - ETA: 0s - loss: 1.9194 - acc: 0.470 - ETA: 0s - loss: 1.9104 - acc: 0.473 - ETA: 0s - loss: 1.9085 - acc: 0.473 - ETA: 0s - loss: 1.9033 - acc: 0.474 - ETA: 0s - loss: 1.9038 - acc: 0.475 - ETA: 0s - loss: 1.8979 - acc: 0.475 - ETA: 0s - loss: 1.8892 - acc: 0.476 - ETA: 0s - loss: 1.8872 - acc: 0.478 - ETA: 0s - loss: 1.8826 - acc: 0.480 - ETA: 0s - loss: 1.8835 - acc: 0.480 - 7s 3ms/step - loss: 1.8740 - acc: 0.4820 - val_loss: 1.7978 - val_acc: 0.5134\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 7s - loss: 1.3115 - acc: 0.656 - ETA: 8s - loss: 1.1592 - acc: 0.718 - ETA: 8s - loss: 1.0625 - acc: 0.760 - ETA: 8s - loss: 1.0801 - acc: 0.734 - ETA: 8s - loss: 1.1836 - acc: 0.712 - ETA: 8s - loss: 1.1998 - acc: 0.697 - ETA: 8s - loss: 1.1514 - acc: 0.705 - ETA: 7s - loss: 1.1896 - acc: 0.683 - ETA: 7s - loss: 1.1554 - acc: 0.691 - ETA: 7s - loss: 1.1723 - acc: 0.681 - ETA: 7s - loss: 1.1526 - acc: 0.684 - ETA: 7s - loss: 1.1765 - acc: 0.674 - ETA: 7s - loss: 1.1577 - acc: 0.677 - ETA: 6s - loss: 1.1564 - acc: 0.680 - ETA: 6s - loss: 1.1355 - acc: 0.685 - ETA: 6s - loss: 1.1326 - acc: 0.687 - ETA: 5s - loss: 1.1288 - acc: 0.691 - ETA: 5s - loss: 1.1130 - acc: 0.694 - ETA: 5s - loss: 1.1077 - acc: 0.690 - ETA: 5s - loss: 1.0919 - acc: 0.693 - ETA: 4s - loss: 1.0932 - acc: 0.692 - ETA: 4s - loss: 1.1084 - acc: 0.688 - ETA: 4s - loss: 1.0977 - acc: 0.691 - ETA: 4s - loss: 1.0819 - acc: 0.696 - ETA: 4s - loss: 1.0762 - acc: 0.698 - ETA: 3s - loss: 1.1002 - acc: 0.693 - ETA: 3s - loss: 1.0997 - acc: 0.695 - ETA: 3s - loss: 1.0931 - acc: 0.696 - ETA: 3s - loss: 1.1066 - acc: 0.688 - ETA: 3s - loss: 1.1054 - acc: 0.688 - ETA: 3s - loss: 1.1099 - acc: 0.690 - ETA: 3s - loss: 1.1120 - acc: 0.689 - ETA: 3s - loss: 1.1157 - acc: 0.688 - ETA: 3s - loss: 1.1247 - acc: 0.687 - ETA: 3s - loss: 1.1154 - acc: 0.689 - ETA: 3s - loss: 1.1140 - acc: 0.689 - ETA: 3s - loss: 1.1071 - acc: 0.691 - ETA: 3s - loss: 1.1120 - acc: 0.690 - ETA: 2s - loss: 1.1125 - acc: 0.691 - ETA: 2s - loss: 1.1122 - acc: 0.689 - ETA: 2s - loss: 1.1087 - acc: 0.688 - ETA: 2s - loss: 1.1098 - acc: 0.688 - ETA: 2s - loss: 1.1057 - acc: 0.688 - ETA: 2s - loss: 1.1033 - acc: 0.686 - ETA: 2s - loss: 1.1082 - acc: 0.684 - ETA: 2s - loss: 1.1014 - acc: 0.686 - ETA: 2s - loss: 1.1153 - acc: 0.682 - ETA: 2s - loss: 1.1214 - acc: 0.680 - ETA: 1s - loss: 1.1160 - acc: 0.680 - ETA: 1s - loss: 1.1164 - acc: 0.680 - ETA: 1s - loss: 1.1197 - acc: 0.679 - ETA: 1s - loss: 1.1144 - acc: 0.679 - ETA: 1s - loss: 1.1087 - acc: 0.679 - ETA: 1s - loss: 1.0967 - acc: 0.683 - ETA: 1s - loss: 1.0900 - acc: 0.684 - ETA: 1s - loss: 1.0914 - acc: 0.683 - ETA: 0s - loss: 1.0904 - acc: 0.682 - ETA: 0s - loss: 1.0917 - acc: 0.682 - ETA: 0s - loss: 1.0880 - acc: 0.683 - ETA: 0s - loss: 1.0869 - acc: 0.683 - ETA: 0s - loss: 1.0852 - acc: 0.682 - ETA: 0s - loss: 1.0897 - acc: 0.682 - ETA: 0s - loss: 1.0989 - acc: 0.679 - ETA: 0s - loss: 1.1030 - acc: 0.676 - ETA: 0s - loss: 1.1038 - acc: 0.676 - 7s 3ms/step - loss: 1.1015 - acc: 0.6765 - val_loss: 1.4278 - val_acc: 0.5780\n",
      "Epoch 5/10\n",
      "2108/2108 [==============================] - ETA: 3s - loss: 0.7925 - acc: 0.687 - ETA: 3s - loss: 0.5887 - acc: 0.781 - ETA: 3s - loss: 0.6223 - acc: 0.770 - ETA: 3s - loss: 0.6587 - acc: 0.781 - ETA: 3s - loss: 0.6884 - acc: 0.768 - ETA: 3s - loss: 0.6884 - acc: 0.770 - ETA: 3s - loss: 0.6900 - acc: 0.767 - ETA: 4s - loss: 0.6661 - acc: 0.777 - ETA: 4s - loss: 0.6755 - acc: 0.781 - ETA: 5s - loss: 0.6675 - acc: 0.784 - ETA: 5s - loss: 0.6700 - acc: 0.778 - ETA: 5s - loss: 0.6464 - acc: 0.794 - ETA: 5s - loss: 0.6860 - acc: 0.783 - ETA: 5s - loss: 0.6719 - acc: 0.790 - ETA: 5s - loss: 0.6692 - acc: 0.789 - ETA: 5s - loss: 0.6952 - acc: 0.781 - ETA: 5s - loss: 0.6980 - acc: 0.783 - ETA: 5s - loss: 0.7066 - acc: 0.784 - ETA: 5s - loss: 0.7098 - acc: 0.782 - ETA: 5s - loss: 0.7129 - acc: 0.779 - ETA: 5s - loss: 0.7070 - acc: 0.782 - ETA: 5s - loss: 0.7173 - acc: 0.781 - ETA: 5s - loss: 0.7118 - acc: 0.784 - ETA: 5s - loss: 0.7193 - acc: 0.782 - ETA: 5s - loss: 0.7245 - acc: 0.780 - ETA: 4s - loss: 0.7265 - acc: 0.775 - ETA: 4s - loss: 0.7293 - acc: 0.776 - ETA: 4s - loss: 0.7188 - acc: 0.776 - ETA: 4s - loss: 0.7196 - acc: 0.775 - ETA: 4s - loss: 0.7131 - acc: 0.778 - ETA: 4s - loss: 0.7089 - acc: 0.778 - ETA: 4s - loss: 0.7122 - acc: 0.775 - ETA: 4s - loss: 0.7038 - acc: 0.777 - ETA: 3s - loss: 0.6964 - acc: 0.779 - ETA: 3s - loss: 0.6921 - acc: 0.778 - ETA: 3s - loss: 0.6883 - acc: 0.777 - ETA: 3s - loss: 0.6875 - acc: 0.778 - ETA: 3s - loss: 0.6893 - acc: 0.778 - ETA: 3s - loss: 0.6820 - acc: 0.779 - ETA: 2s - loss: 0.6776 - acc: 0.780 - ETA: 2s - loss: 0.6837 - acc: 0.779 - ETA: 2s - loss: 0.6849 - acc: 0.779 - ETA: 2s - loss: 0.6833 - acc: 0.780 - ETA: 2s - loss: 0.6817 - acc: 0.780 - ETA: 2s - loss: 0.6844 - acc: 0.780 - ETA: 2s - loss: 0.6797 - acc: 0.782 - ETA: 2s - loss: 0.6791 - acc: 0.783 - ETA: 2s - loss: 0.6761 - acc: 0.783 - ETA: 1s - loss: 0.6816 - acc: 0.781 - ETA: 1s - loss: 0.6747 - acc: 0.783 - ETA: 1s - loss: 0.6739 - acc: 0.783 - ETA: 1s - loss: 0.6817 - acc: 0.781 - ETA: 1s - loss: 0.6829 - acc: 0.781 - ETA: 1s - loss: 0.6831 - acc: 0.781 - ETA: 1s - loss: 0.6778 - acc: 0.783 - ETA: 1s - loss: 0.6772 - acc: 0.783 - ETA: 1s - loss: 0.6740 - acc: 0.783 - ETA: 0s - loss: 0.6714 - acc: 0.784 - ETA: 0s - loss: 0.6746 - acc: 0.783 - ETA: 0s - loss: 0.6827 - acc: 0.781 - ETA: 0s - loss: 0.6809 - acc: 0.782 - ETA: 0s - loss: 0.6882 - acc: 0.780 - ETA: 0s - loss: 0.6875 - acc: 0.781 - ETA: 0s - loss: 0.6849 - acc: 0.783 - ETA: 0s - loss: 0.6899 - acc: 0.783 - 8s 4ms/step - loss: 0.6875 - acc: 0.7837 - val_loss: 1.2417 - val_acc: 0.6882\n",
      "Epoch 6/10\n",
      "2108/2108 [==============================] - ETA: 3s - loss: 0.2781 - acc: 0.968 - ETA: 3s - loss: 0.3415 - acc: 0.953 - ETA: 3s - loss: 0.3349 - acc: 0.947 - ETA: 3s - loss: 0.3151 - acc: 0.960 - ETA: 5s - loss: 0.3292 - acc: 0.931 - ETA: 5s - loss: 0.3570 - acc: 0.911 - ETA: 5s - loss: 0.3652 - acc: 0.906 - ETA: 6s - loss: 0.3756 - acc: 0.910 - ETA: 6s - loss: 0.3756 - acc: 0.909 - ETA: 6s - loss: 0.4235 - acc: 0.884 - ETA: 6s - loss: 0.4064 - acc: 0.886 - ETA: 6s - loss: 0.4300 - acc: 0.872 - ETA: 6s - loss: 0.4282 - acc: 0.870 - ETA: 6s - loss: 0.4476 - acc: 0.866 - ETA: 6s - loss: 0.4412 - acc: 0.868 - ETA: 6s - loss: 0.4223 - acc: 0.877 - ETA: 6s - loss: 0.4253 - acc: 0.875 - ETA: 6s - loss: 0.4244 - acc: 0.876 - ETA: 5s - loss: 0.4163 - acc: 0.878 - ETA: 5s - loss: 0.4180 - acc: 0.878 - ETA: 5s - loss: 0.4282 - acc: 0.872 - ETA: 5s - loss: 0.4255 - acc: 0.873 - ETA: 5s - loss: 0.4267 - acc: 0.875 - ETA: 5s - loss: 0.4227 - acc: 0.876 - ETA: 5s - loss: 0.4380 - acc: 0.872 - ETA: 5s - loss: 0.4538 - acc: 0.865 - ETA: 5s - loss: 0.4460 - acc: 0.868 - ETA: 5s - loss: 0.4448 - acc: 0.867 - ETA: 4s - loss: 0.4517 - acc: 0.867 - ETA: 4s - loss: 0.4550 - acc: 0.864 - ETA: 4s - loss: 0.4478 - acc: 0.864 - ETA: 4s - loss: 0.4413 - acc: 0.867 - ETA: 4s - loss: 0.4427 - acc: 0.867 - ETA: 3s - loss: 0.4410 - acc: 0.868 - ETA: 3s - loss: 0.4398 - acc: 0.868 - ETA: 3s - loss: 0.4450 - acc: 0.867 - ETA: 3s - loss: 0.4489 - acc: 0.866 - ETA: 3s - loss: 0.4568 - acc: 0.865 - ETA: 3s - loss: 0.4504 - acc: 0.866 - ETA: 3s - loss: 0.4491 - acc: 0.865 - ETA: 2s - loss: 0.4495 - acc: 0.865 - ETA: 2s - loss: 0.4463 - acc: 0.866 - ETA: 2s - loss: 0.4470 - acc: 0.867 - ETA: 2s - loss: 0.4417 - acc: 0.869 - ETA: 2s - loss: 0.4446 - acc: 0.868 - ETA: 2s - loss: 0.4537 - acc: 0.864 - ETA: 2s - loss: 0.4519 - acc: 0.864 - ETA: 2s - loss: 0.4451 - acc: 0.867 - ETA: 2s - loss: 0.4451 - acc: 0.866 - ETA: 1s - loss: 0.4426 - acc: 0.866 - ETA: 1s - loss: 0.4401 - acc: 0.867 - ETA: 1s - loss: 0.4351 - acc: 0.867 - ETA: 1s - loss: 0.4360 - acc: 0.867 - ETA: 1s - loss: 0.4376 - acc: 0.866 - ETA: 1s - loss: 0.4358 - acc: 0.867 - ETA: 1s - loss: 0.4372 - acc: 0.867 - ETA: 1s - loss: 0.4416 - acc: 0.865 - ETA: 1s - loss: 0.4472 - acc: 0.862 - ETA: 0s - loss: 0.4427 - acc: 0.863 - ETA: 0s - loss: 0.4441 - acc: 0.863 - ETA: 0s - loss: 0.4450 - acc: 0.862 - ETA: 0s - loss: 0.4404 - acc: 0.864 - ETA: 0s - loss: 0.4405 - acc: 0.864 - ETA: 0s - loss: 0.4467 - acc: 0.860 - ETA: 0s - loss: 0.4532 - acc: 0.858 - 9s 4ms/step - loss: 0.4592 - acc: 0.8567 - val_loss: 1.2757 - val_acc: 0.6828\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 8s - loss: 0.1754 - acc: 0.968 - ETA: 8s - loss: 0.2569 - acc: 0.937 - ETA: 8s - loss: 0.2755 - acc: 0.916 - ETA: 8s - loss: 0.2868 - acc: 0.921 - ETA: 8s - loss: 0.2802 - acc: 0.925 - ETA: 8s - loss: 0.2895 - acc: 0.916 - ETA: 8s - loss: 0.3182 - acc: 0.906 - ETA: 8s - loss: 0.3130 - acc: 0.914 - ETA: 7s - loss: 0.3138 - acc: 0.909 - ETA: 7s - loss: 0.3261 - acc: 0.896 - ETA: 7s - loss: 0.3191 - acc: 0.900 - ETA: 7s - loss: 0.3275 - acc: 0.898 - ETA: 7s - loss: 0.3140 - acc: 0.903 - ETA: 7s - loss: 0.3225 - acc: 0.899 - ETA: 7s - loss: 0.3093 - acc: 0.904 - ETA: 7s - loss: 0.3125 - acc: 0.902 - ETA: 6s - loss: 0.3054 - acc: 0.902 - ETA: 6s - loss: 0.2984 - acc: 0.906 - ETA: 6s - loss: 0.2908 - acc: 0.907 - ETA: 6s - loss: 0.2852 - acc: 0.909 - ETA: 6s - loss: 0.2780 - acc: 0.912 - ETA: 6s - loss: 0.2783 - acc: 0.910 - ETA: 5s - loss: 0.2791 - acc: 0.909 - ETA: 5s - loss: 0.2750 - acc: 0.911 - ETA: 5s - loss: 0.2712 - acc: 0.912 - ETA: 5s - loss: 0.2756 - acc: 0.912 - ETA: 4s - loss: 0.2774 - acc: 0.910 - ETA: 4s - loss: 0.2798 - acc: 0.909 - ETA: 4s - loss: 0.2799 - acc: 0.910 - ETA: 4s - loss: 0.2750 - acc: 0.912 - ETA: 4s - loss: 0.2787 - acc: 0.911 - ETA: 3s - loss: 0.2795 - acc: 0.910 - ETA: 3s - loss: 0.2786 - acc: 0.910 - ETA: 3s - loss: 0.2830 - acc: 0.909 - ETA: 3s - loss: 0.2818 - acc: 0.908 - ETA: 3s - loss: 0.2839 - acc: 0.910 - ETA: 3s - loss: 0.2797 - acc: 0.912 - ETA: 3s - loss: 0.2791 - acc: 0.913 - ETA: 3s - loss: 0.2788 - acc: 0.913 - ETA: 3s - loss: 0.2898 - acc: 0.909 - ETA: 3s - loss: 0.2916 - acc: 0.907 - ETA: 2s - loss: 0.2993 - acc: 0.904 - ETA: 2s - loss: 0.3040 - acc: 0.899 - ETA: 2s - loss: 0.3079 - acc: 0.897 - ETA: 2s - loss: 0.3072 - acc: 0.897 - ETA: 2s - loss: 0.3137 - acc: 0.894 - ETA: 2s - loss: 0.3207 - acc: 0.891 - ETA: 2s - loss: 0.3222 - acc: 0.891 - ETA: 2s - loss: 0.3226 - acc: 0.892 - ETA: 2s - loss: 0.3247 - acc: 0.891 - ETA: 1s - loss: 0.3272 - acc: 0.890 - ETA: 1s - loss: 0.3245 - acc: 0.892 - ETA: 1s - loss: 0.3316 - acc: 0.890 - ETA: 1s - loss: 0.3441 - acc: 0.887 - ETA: 1s - loss: 0.3447 - acc: 0.887 - ETA: 1s - loss: 0.3499 - acc: 0.886 - ETA: 1s - loss: 0.3527 - acc: 0.884 - ETA: 0s - loss: 0.3570 - acc: 0.883 - ETA: 0s - loss: 0.3566 - acc: 0.883 - ETA: 0s - loss: 0.3573 - acc: 0.883 - ETA: 0s - loss: 0.3585 - acc: 0.883 - ETA: 0s - loss: 0.3613 - acc: 0.882 - ETA: 0s - loss: 0.3613 - acc: 0.882 - ETA: 0s - loss: 0.3599 - acc: 0.882 - ETA: 0s - loss: 0.3610 - acc: 0.882 - 9s 4ms/step - loss: 0.3639 - acc: 0.8814 - val_loss: 1.4581 - val_acc: 0.6317\n",
      "Epoch 8/10\n",
      "2108/2108 [==============================] - ETA: 8s - loss: 0.1119 - acc: 1.000 - ETA: 8s - loss: 0.2516 - acc: 0.921 - ETA: 8s - loss: 0.3773 - acc: 0.885 - ETA: 8s - loss: 0.3151 - acc: 0.914 - ETA: 8s - loss: 0.3045 - acc: 0.918 - ETA: 8s - loss: 0.3272 - acc: 0.911 - ETA: 8s - loss: 0.3498 - acc: 0.906 - ETA: 8s - loss: 0.3509 - acc: 0.902 - ETA: 8s - loss: 0.3409 - acc: 0.906 - ETA: 7s - loss: 0.3254 - acc: 0.909 - ETA: 7s - loss: 0.3153 - acc: 0.909 - ETA: 7s - loss: 0.3151 - acc: 0.908 - ETA: 7s - loss: 0.3156 - acc: 0.906 - ETA: 7s - loss: 0.2991 - acc: 0.910 - ETA: 7s - loss: 0.2877 - acc: 0.914 - ETA: 7s - loss: 0.2814 - acc: 0.919 - ETA: 7s - loss: 0.2747 - acc: 0.922 - ETA: 6s - loss: 0.2782 - acc: 0.921 - ETA: 6s - loss: 0.2881 - acc: 0.917 - ETA: 6s - loss: 0.2777 - acc: 0.921 - ETA: 5s - loss: 0.2701 - acc: 0.922 - ETA: 5s - loss: 0.2649 - acc: 0.924 - ETA: 5s - loss: 0.2583 - acc: 0.928 - ETA: 5s - loss: 0.2626 - acc: 0.927 - ETA: 4s - loss: 0.2565 - acc: 0.927 - ETA: 4s - loss: 0.2561 - acc: 0.927 - ETA: 4s - loss: 0.2508 - acc: 0.929 - ETA: 4s - loss: 0.2461 - acc: 0.930 - ETA: 4s - loss: 0.2476 - acc: 0.932 - ETA: 3s - loss: 0.2466 - acc: 0.931 - ETA: 3s - loss: 0.2480 - acc: 0.930 - ETA: 3s - loss: 0.2473 - acc: 0.928 - ETA: 3s - loss: 0.2461 - acc: 0.929 - ETA: 3s - loss: 0.2422 - acc: 0.929 - ETA: 3s - loss: 0.2369 - acc: 0.931 - ETA: 3s - loss: 0.2369 - acc: 0.930 - ETA: 3s - loss: 0.2364 - acc: 0.929 - ETA: 3s - loss: 0.2349 - acc: 0.929 - ETA: 3s - loss: 0.2350 - acc: 0.928 - ETA: 3s - loss: 0.2380 - acc: 0.928 - ETA: 2s - loss: 0.2380 - acc: 0.928 - ETA: 2s - loss: 0.2386 - acc: 0.926 - ETA: 2s - loss: 0.2375 - acc: 0.926 - ETA: 2s - loss: 0.2361 - acc: 0.927 - ETA: 2s - loss: 0.2344 - acc: 0.928 - ETA: 2s - loss: 0.2316 - acc: 0.929 - ETA: 2s - loss: 0.2324 - acc: 0.928 - ETA: 2s - loss: 0.2299 - acc: 0.929 - ETA: 2s - loss: 0.2291 - acc: 0.929 - ETA: 1s - loss: 0.2272 - acc: 0.929 - ETA: 1s - loss: 0.2291 - acc: 0.927 - ETA: 1s - loss: 0.2351 - acc: 0.926 - ETA: 1s - loss: 0.2336 - acc: 0.926 - ETA: 1s - loss: 0.2310 - acc: 0.927 - ETA: 1s - loss: 0.2320 - acc: 0.927 - ETA: 1s - loss: 0.2299 - acc: 0.927 - ETA: 1s - loss: 0.2295 - acc: 0.927 - ETA: 0s - loss: 0.2273 - acc: 0.928 - ETA: 0s - loss: 0.2247 - acc: 0.929 - ETA: 0s - loss: 0.2238 - acc: 0.930 - ETA: 0s - loss: 0.2215 - acc: 0.930 - ETA: 0s - loss: 0.2302 - acc: 0.929 - ETA: 0s - loss: 0.2298 - acc: 0.929 - ETA: 0s - loss: 0.2307 - acc: 0.928 - ETA: 0s - loss: 0.2279 - acc: 0.929 - 8s 4ms/step - loss: 0.2320 - acc: 0.9274 - val_loss: 1.3030 - val_acc: 0.6801\n",
      "Epoch 9/10\n",
      "2108/2108 [==============================] - ETA: 3s - loss: 0.0593 - acc: 1.000 - ETA: 3s - loss: 0.1066 - acc: 0.984 - ETA: 3s - loss: 0.1386 - acc: 0.968 - ETA: 3s - loss: 0.1327 - acc: 0.968 - ETA: 3s - loss: 0.1537 - acc: 0.956 - ETA: 3s - loss: 0.1463 - acc: 0.953 - ETA: 3s - loss: 0.1558 - acc: 0.950 - ETA: 3s - loss: 0.1414 - acc: 0.957 - ETA: 3s - loss: 0.1420 - acc: 0.958 - ETA: 3s - loss: 0.1563 - acc: 0.959 - ETA: 3s - loss: 0.1623 - acc: 0.957 - ETA: 4s - loss: 0.1526 - acc: 0.960 - ETA: 4s - loss: 0.1481 - acc: 0.963 - ETA: 4s - loss: 0.1442 - acc: 0.964 - ETA: 4s - loss: 0.1445 - acc: 0.962 - ETA: 4s - loss: 0.1418 - acc: 0.962 - ETA: 4s - loss: 0.1409 - acc: 0.961 - ETA: 4s - loss: 0.1428 - acc: 0.960 - ETA: 4s - loss: 0.1413 - acc: 0.958 - ETA: 4s - loss: 0.1458 - acc: 0.956 - ETA: 4s - loss: 0.1458 - acc: 0.953 - ETA: 4s - loss: 0.1469 - acc: 0.953 - ETA: 4s - loss: 0.1473 - acc: 0.952 - ETA: 4s - loss: 0.1449 - acc: 0.954 - ETA: 4s - loss: 0.1416 - acc: 0.955 - ETA: 4s - loss: 0.1506 - acc: 0.953 - ETA: 4s - loss: 0.1476 - acc: 0.953 - ETA: 4s - loss: 0.1469 - acc: 0.954 - ETA: 4s - loss: 0.1453 - acc: 0.953 - ETA: 4s - loss: 0.1462 - acc: 0.953 - ETA: 4s - loss: 0.1465 - acc: 0.952 - ETA: 4s - loss: 0.1492 - acc: 0.951 - ETA: 3s - loss: 0.1562 - acc: 0.948 - ETA: 3s - loss: 0.1534 - acc: 0.950 - ETA: 3s - loss: 0.1520 - acc: 0.950 - ETA: 3s - loss: 0.1541 - acc: 0.950 - ETA: 3s - loss: 0.1521 - acc: 0.951 - ETA: 3s - loss: 0.1512 - acc: 0.952 - ETA: 2s - loss: 0.1514 - acc: 0.951 - ETA: 2s - loss: 0.1525 - acc: 0.950 - ETA: 2s - loss: 0.1506 - acc: 0.950 - ETA: 2s - loss: 0.1522 - acc: 0.949 - ETA: 2s - loss: 0.1505 - acc: 0.950 - ETA: 2s - loss: 0.1524 - acc: 0.951 - ETA: 2s - loss: 0.1500 - acc: 0.952 - ETA: 2s - loss: 0.1480 - acc: 0.952 - ETA: 1s - loss: 0.1477 - acc: 0.952 - ETA: 1s - loss: 0.1505 - acc: 0.950 - ETA: 1s - loss: 0.1509 - acc: 0.950 - ETA: 1s - loss: 0.1530 - acc: 0.949 - ETA: 1s - loss: 0.1529 - acc: 0.949 - ETA: 1s - loss: 0.1542 - acc: 0.948 - ETA: 1s - loss: 0.1592 - acc: 0.946 - ETA: 1s - loss: 0.1609 - acc: 0.946 - ETA: 1s - loss: 0.1619 - acc: 0.946 - ETA: 1s - loss: 0.1620 - acc: 0.945 - ETA: 0s - loss: 0.1635 - acc: 0.945 - ETA: 0s - loss: 0.1618 - acc: 0.946 - ETA: 0s - loss: 0.1621 - acc: 0.946 - ETA: 0s - loss: 0.1653 - acc: 0.945 - ETA: 0s - loss: 0.1637 - acc: 0.946 - ETA: 0s - loss: 0.1661 - acc: 0.945 - ETA: 0s - loss: 0.1726 - acc: 0.943 - ETA: 0s - loss: 0.1712 - acc: 0.943 - ETA: 0s - loss: 0.1694 - acc: 0.944 - 8s 4ms/step - loss: 0.1729 - acc: 0.9435 - val_loss: 1.3920 - val_acc: 0.6935\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 13s - loss: 0.1008 - acc: 0.96 - ETA: 10s - loss: 0.1488 - acc: 0.95 - ETA: 9s - loss: 0.1572 - acc: 0.9375 - ETA: 8s - loss: 0.1708 - acc: 0.937 - ETA: 8s - loss: 0.1773 - acc: 0.937 - ETA: 8s - loss: 0.1822 - acc: 0.942 - ETA: 7s - loss: 0.1785 - acc: 0.942 - ETA: 7s - loss: 0.1893 - acc: 0.933 - ETA: 6s - loss: 0.1786 - acc: 0.941 - ETA: 6s - loss: 0.1895 - acc: 0.940 - ETA: 5s - loss: 0.1733 - acc: 0.946 - ETA: 5s - loss: 0.1834 - acc: 0.940 - ETA: 5s - loss: 0.1704 - acc: 0.944 - ETA: 5s - loss: 0.1635 - acc: 0.946 - ETA: 4s - loss: 0.1552 - acc: 0.950 - ETA: 4s - loss: 0.1535 - acc: 0.949 - ETA: 4s - loss: 0.1569 - acc: 0.948 - ETA: 4s - loss: 0.1502 - acc: 0.951 - ETA: 4s - loss: 0.1598 - acc: 0.949 - ETA: 4s - loss: 0.1546 - acc: 0.950 - ETA: 4s - loss: 0.1514 - acc: 0.950 - ETA: 4s - loss: 0.1455 - acc: 0.953 - ETA: 4s - loss: 0.1452 - acc: 0.953 - ETA: 4s - loss: 0.1467 - acc: 0.954 - ETA: 4s - loss: 0.1441 - acc: 0.956 - ETA: 4s - loss: 0.1419 - acc: 0.956 - ETA: 4s - loss: 0.1446 - acc: 0.956 - ETA: 4s - loss: 0.1429 - acc: 0.956 - ETA: 4s - loss: 0.1420 - acc: 0.955 - ETA: 4s - loss: 0.1462 - acc: 0.953 - ETA: 3s - loss: 0.1430 - acc: 0.954 - ETA: 3s - loss: 0.1427 - acc: 0.954 - ETA: 3s - loss: 0.1406 - acc: 0.953 - ETA: 3s - loss: 0.1373 - acc: 0.955 - ETA: 3s - loss: 0.1349 - acc: 0.955 - ETA: 3s - loss: 0.1323 - acc: 0.956 - ETA: 3s - loss: 0.1291 - acc: 0.957 - ETA: 3s - loss: 0.1332 - acc: 0.958 - ETA: 3s - loss: 0.1310 - acc: 0.959 - ETA: 3s - loss: 0.1286 - acc: 0.960 - ETA: 3s - loss: 0.1265 - acc: 0.961 - ETA: 2s - loss: 0.1313 - acc: 0.958 - ETA: 2s - loss: 0.1368 - acc: 0.957 - ETA: 2s - loss: 0.1374 - acc: 0.957 - ETA: 2s - loss: 0.1350 - acc: 0.958 - ETA: 2s - loss: 0.1340 - acc: 0.957 - ETA: 2s - loss: 0.1403 - acc: 0.955 - ETA: 2s - loss: 0.1378 - acc: 0.956 - ETA: 1s - loss: 0.1358 - acc: 0.957 - ETA: 1s - loss: 0.1357 - acc: 0.957 - ETA: 1s - loss: 0.1338 - acc: 0.958 - ETA: 1s - loss: 0.1323 - acc: 0.959 - ETA: 1s - loss: 0.1311 - acc: 0.959 - ETA: 1s - loss: 0.1306 - acc: 0.960 - ETA: 1s - loss: 0.1289 - acc: 0.960 - ETA: 1s - loss: 0.1340 - acc: 0.960 - ETA: 1s - loss: 0.1355 - acc: 0.960 - ETA: 0s - loss: 0.1336 - acc: 0.960 - ETA: 0s - loss: 0.1329 - acc: 0.960 - ETA: 0s - loss: 0.1340 - acc: 0.959 - ETA: 0s - loss: 0.1325 - acc: 0.960 - ETA: 0s - loss: 0.1319 - acc: 0.960 - ETA: 0s - loss: 0.1330 - acc: 0.959 - ETA: 0s - loss: 0.1332 - acc: 0.958 - ETA: 0s - loss: 0.1325 - acc: 0.958 - 9s 4ms/step - loss: 0.1315 - acc: 0.9592 - val_loss: 1.4363 - val_acc: 0.7043\n",
      "Final accuracy on the training data is: 0.959203036053131\n",
      "Cross_Validating accuracy is: 0.7043010752688172\n"
     ]
    }
   ],
   "source": [
    "model = Sequential()\n",
    "model.add(Conv2D(124, kernel_size=(3, 3), activation='relu', input_shape=(h,w,d)))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(248, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Conv2D(400, kernel_size=(3, 3), activation='relu'))\n",
    "model.add(MaxPooling2D(pool_size=(2, 2)))\n",
    "model.add(Flatten())\n",
    "model.add(Dense(248, activation='relu'))\n",
    "model.add(Dense(124, activation='relu'))\n",
    "model.add(Dense(62, activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "feeds=model.fit(x=X_train_arr,y=y_train_arr,epochs=10,validation_data=(X_val_arr,y_val_arr))\n",
    "#print(feeds.history)\n",
    "print(\"Final accuracy on the training data is:\",feeds.history['acc'][-1]);\n",
    "print(\"Cross_Validating accuracy is:\",feeds.history['val_acc'][-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Final readings are promising but again complexity is more I lastly tried a simple model which led to a low cross_validation accuracy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# MODEL 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:2: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(248, kernel_size=5, activation=\"relu\", input_shape=(50, 50, 1..., padding=\"same\")`\n",
      "  \n",
      "D:\\anaconda\\lib\\site-packages\\ipykernel_launcher.py:4: UserWarning: Update your `Conv2D` call to the Keras 2 API: `Conv2D(62, kernel_size=3, activation=\"relu\", padding=\"same\")`\n",
      "  after removing the cwd from sys.path.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 2108 samples, validate on 372 samples\n",
      "Epoch 1/10\n",
      "2108/2108 [==============================] - ETA: 1:56 - loss: 4.1337 - acc: 0.0000e+0 - ETA: 58s - loss: 4.1254 - acc: 0.0156    - ETA: 39s - loss: 4.1859 - acc: 0.01 - ETA: 30s - loss: 4.1586 - acc: 0.01 - ETA: 24s - loss: 4.1543 - acc: 0.01 - ETA: 20s - loss: 4.1535 - acc: 0.01 - ETA: 17s - loss: 4.1514 - acc: 0.01 - ETA: 15s - loss: 4.1479 - acc: 0.01 - ETA: 13s - loss: 4.1458 - acc: 0.01 - ETA: 12s - loss: 4.1435 - acc: 0.01 - ETA: 11s - loss: 4.1422 - acc: 0.01 - ETA: 9s - loss: 4.1387 - acc: 0.0168 - ETA: 9s - loss: 4.1374 - acc: 0.015 - ETA: 8s - loss: 4.1362 - acc: 0.018 - ETA: 7s - loss: 4.1349 - acc: 0.023 - ETA: 7s - loss: 4.1337 - acc: 0.023 - ETA: 6s - loss: 4.1307 - acc: 0.023 - ETA: 5s - loss: 4.1289 - acc: 0.022 - ETA: 5s - loss: 4.1274 - acc: 0.021 - ETA: 5s - loss: 4.1262 - acc: 0.020 - ETA: 4s - loss: 4.1227 - acc: 0.025 - ETA: 4s - loss: 4.1217 - acc: 0.024 - ETA: 4s - loss: 4.1195 - acc: 0.022 - ETA: 3s - loss: 4.1140 - acc: 0.028 - ETA: 3s - loss: 4.1099 - acc: 0.027 - ETA: 3s - loss: 4.1025 - acc: 0.030 - ETA: 3s - loss: 4.0991 - acc: 0.032 - ETA: 2s - loss: 4.0959 - acc: 0.032 - ETA: 2s - loss: 4.0902 - acc: 0.031 - ETA: 2s - loss: 4.0807 - acc: 0.030 - ETA: 2s - loss: 4.0788 - acc: 0.029 - ETA: 2s - loss: 4.0750 - acc: 0.029 - ETA: 2s - loss: 4.0677 - acc: 0.031 - ETA: 1s - loss: 4.0590 - acc: 0.032 - ETA: 1s - loss: 4.0533 - acc: 0.031 - ETA: 1s - loss: 4.0460 - acc: 0.036 - ETA: 1s - loss: 4.0387 - acc: 0.037 - ETA: 1s - loss: 4.0160 - acc: 0.043 - ETA: 1s - loss: 4.0075 - acc: 0.044 - ETA: 1s - loss: 3.9857 - acc: 0.049 - ETA: 0s - loss: 3.9763 - acc: 0.052 - ETA: 0s - loss: 3.9647 - acc: 0.055 - ETA: 0s - loss: 3.9516 - acc: 0.058 - ETA: 0s - loss: 3.9448 - acc: 0.058 - ETA: 0s - loss: 3.9369 - acc: 0.059 - ETA: 0s - loss: 3.9285 - acc: 0.060 - ETA: 0s - loss: 3.9157 - acc: 0.060 - ETA: 0s - loss: 3.9060 - acc: 0.063 - ETA: 0s - loss: 3.8851 - acc: 0.066 - ETA: 0s - loss: 3.8591 - acc: 0.072 - 6s 3ms/step - loss: 3.8511 - acc: 0.0745 - val_loss: 3.2377 - val_acc: 0.1962\n",
      "Epoch 2/10\n",
      "2108/2108 [==============================] - ETA: 3s - loss: 3.0804 - acc: 0.312 - ETA: 3s - loss: 2.9978 - acc: 0.281 - ETA: 3s - loss: 3.0381 - acc: 0.281 - ETA: 3s - loss: 3.0080 - acc: 0.250 - ETA: 3s - loss: 2.9778 - acc: 0.256 - ETA: 3s - loss: 2.9509 - acc: 0.265 - ETA: 3s - loss: 2.9219 - acc: 0.272 - ETA: 3s - loss: 2.8379 - acc: 0.293 - ETA: 3s - loss: 2.8411 - acc: 0.309 - ETA: 3s - loss: 2.7993 - acc: 0.312 - ETA: 4s - loss: 2.7958 - acc: 0.318 - ETA: 4s - loss: 2.7703 - acc: 0.317 - ETA: 4s - loss: 2.7347 - acc: 0.331 - ETA: 4s - loss: 2.7301 - acc: 0.341 - ETA: 4s - loss: 2.7369 - acc: 0.337 - ETA: 4s - loss: 2.7298 - acc: 0.334 - ETA: 4s - loss: 2.7081 - acc: 0.338 - ETA: 4s - loss: 2.6967 - acc: 0.340 - ETA: 4s - loss: 2.6879 - acc: 0.343 - ETA: 4s - loss: 2.6701 - acc: 0.350 - ETA: 4s - loss: 2.6612 - acc: 0.354 - ETA: 4s - loss: 2.6330 - acc: 0.358 - ETA: 4s - loss: 2.6361 - acc: 0.361 - ETA: 4s - loss: 2.6390 - acc: 0.362 - ETA: 4s - loss: 2.6396 - acc: 0.356 - ETA: 4s - loss: 2.6407 - acc: 0.354 - ETA: 3s - loss: 2.6343 - acc: 0.351 - ETA: 3s - loss: 2.6247 - acc: 0.356 - ETA: 3s - loss: 2.6202 - acc: 0.356 - ETA: 3s - loss: 2.6474 - acc: 0.357 - ETA: 3s - loss: 2.6327 - acc: 0.359 - ETA: 3s - loss: 2.6308 - acc: 0.357 - ETA: 3s - loss: 2.6255 - acc: 0.358 - ETA: 3s - loss: 2.6299 - acc: 0.361 - ETA: 3s - loss: 2.6351 - acc: 0.359 - ETA: 3s - loss: 2.6304 - acc: 0.361 - ETA: 3s - loss: 2.6267 - acc: 0.362 - ETA: 2s - loss: 2.6176 - acc: 0.362 - ETA: 2s - loss: 2.6109 - acc: 0.359 - ETA: 2s - loss: 2.6059 - acc: 0.360 - ETA: 2s - loss: 2.5995 - acc: 0.364 - ETA: 2s - loss: 2.6043 - acc: 0.363 - ETA: 2s - loss: 2.5902 - acc: 0.366 - ETA: 1s - loss: 2.5790 - acc: 0.368 - ETA: 1s - loss: 2.5783 - acc: 0.371 - ETA: 1s - loss: 2.5749 - acc: 0.371 - ETA: 1s - loss: 2.5745 - acc: 0.371 - ETA: 1s - loss: 2.5661 - acc: 0.373 - ETA: 1s - loss: 2.5609 - acc: 0.376 - ETA: 1s - loss: 2.5558 - acc: 0.378 - ETA: 1s - loss: 2.5472 - acc: 0.380 - ETA: 1s - loss: 2.5572 - acc: 0.378 - ETA: 0s - loss: 2.5546 - acc: 0.379 - ETA: 0s - loss: 2.5660 - acc: 0.376 - ETA: 0s - loss: 2.5624 - acc: 0.377 - ETA: 0s - loss: 2.5579 - acc: 0.378 - ETA: 0s - loss: 2.5502 - acc: 0.380 - ETA: 0s - loss: 2.5465 - acc: 0.381 - ETA: 0s - loss: 2.5434 - acc: 0.380 - ETA: 0s - loss: 2.5349 - acc: 0.382 - ETA: 0s - loss: 2.5297 - acc: 0.382 - ETA: 0s - loss: 2.5222 - acc: 0.385 - ETA: 0s - loss: 2.5290 - acc: 0.386 - 7s 3ms/step - loss: 2.5318 - acc: 0.3871 - val_loss: 2.7333 - val_acc: 0.3575\n",
      "Epoch 3/10\n",
      "2108/2108 [==============================] - ETA: 6s - loss: 1.7407 - acc: 0.531 - ETA: 7s - loss: 2.0309 - acc: 0.468 - ETA: 7s - loss: 1.9896 - acc: 0.500 - ETA: 7s - loss: 1.9294 - acc: 0.515 - ETA: 7s - loss: 1.8731 - acc: 0.518 - ETA: 7s - loss: 1.8692 - acc: 0.526 - ETA: 6s - loss: 1.9137 - acc: 0.517 - ETA: 6s - loss: 1.9867 - acc: 0.500 - ETA: 6s - loss: 1.9256 - acc: 0.510 - ETA: 6s - loss: 1.9192 - acc: 0.518 - ETA: 6s - loss: 1.9396 - acc: 0.505 - ETA: 6s - loss: 1.9759 - acc: 0.505 - ETA: 6s - loss: 1.9622 - acc: 0.502 - ETA: 6s - loss: 1.9459 - acc: 0.506 - ETA: 6s - loss: 1.9695 - acc: 0.497 - ETA: 6s - loss: 1.9586 - acc: 0.503 - ETA: 5s - loss: 1.9598 - acc: 0.498 - ETA: 5s - loss: 1.9374 - acc: 0.503 - ETA: 5s - loss: 1.9283 - acc: 0.503 - ETA: 5s - loss: 1.9271 - acc: 0.503 - ETA: 4s - loss: 1.9234 - acc: 0.510 - ETA: 4s - loss: 1.8999 - acc: 0.517 - ETA: 4s - loss: 1.9056 - acc: 0.514 - ETA: 4s - loss: 1.8845 - acc: 0.515 - ETA: 4s - loss: 1.8895 - acc: 0.510 - ETA: 3s - loss: 1.8784 - acc: 0.512 - ETA: 3s - loss: 1.8744 - acc: 0.512 - ETA: 3s - loss: 1.8640 - acc: 0.513 - ETA: 3s - loss: 1.8782 - acc: 0.508 - ETA: 3s - loss: 1.8655 - acc: 0.511 - ETA: 3s - loss: 1.8494 - acc: 0.514 - ETA: 3s - loss: 1.8416 - acc: 0.514 - ETA: 3s - loss: 1.8426 - acc: 0.510 - ETA: 3s - loss: 1.8386 - acc: 0.511 - ETA: 3s - loss: 1.8332 - acc: 0.515 - ETA: 3s - loss: 1.8447 - acc: 0.513 - ETA: 3s - loss: 1.8547 - acc: 0.511 - ETA: 2s - loss: 1.8337 - acc: 0.515 - ETA: 2s - loss: 1.8305 - acc: 0.516 - ETA: 2s - loss: 1.8404 - acc: 0.514 - ETA: 2s - loss: 1.8499 - acc: 0.516 - ETA: 2s - loss: 1.8531 - acc: 0.518 - ETA: 2s - loss: 1.8503 - acc: 0.521 - ETA: 2s - loss: 1.8525 - acc: 0.521 - ETA: 2s - loss: 1.8506 - acc: 0.523 - ETA: 2s - loss: 1.8541 - acc: 0.525 - ETA: 2s - loss: 1.8584 - acc: 0.522 - ETA: 1s - loss: 1.8577 - acc: 0.525 - ETA: 1s - loss: 1.8543 - acc: 0.524 - ETA: 1s - loss: 1.8572 - acc: 0.523 - ETA: 1s - loss: 1.8611 - acc: 0.523 - ETA: 1s - loss: 1.8638 - acc: 0.522 - ETA: 1s - loss: 1.8570 - acc: 0.524 - ETA: 1s - loss: 1.8571 - acc: 0.526 - ETA: 1s - loss: 1.8550 - acc: 0.525 - ETA: 1s - loss: 1.8437 - acc: 0.530 - ETA: 0s - loss: 1.8426 - acc: 0.530 - ETA: 0s - loss: 1.8365 - acc: 0.532 - ETA: 0s - loss: 1.8435 - acc: 0.529 - ETA: 0s - loss: 1.8488 - acc: 0.529 - ETA: 0s - loss: 1.8401 - acc: 0.531 - ETA: 0s - loss: 1.8494 - acc: 0.530 - ETA: 0s - loss: 1.8632 - acc: 0.528 - ETA: 0s - loss: 1.8701 - acc: 0.527 - ETA: 0s - loss: 1.8687 - acc: 0.527 - 7s 3ms/step - loss: 1.8689 - acc: 0.5266 - val_loss: 2.4536 - val_acc: 0.4489\n",
      "Epoch 4/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 6s - loss: 2.1497 - acc: 0.437 - ETA: 7s - loss: 1.7484 - acc: 0.531 - ETA: 7s - loss: 1.6304 - acc: 0.614 - ETA: 7s - loss: 1.5184 - acc: 0.656 - ETA: 7s - loss: 1.5482 - acc: 0.643 - ETA: 6s - loss: 1.5637 - acc: 0.625 - ETA: 6s - loss: 1.5567 - acc: 0.616 - ETA: 6s - loss: 1.5512 - acc: 0.621 - ETA: 6s - loss: 1.5453 - acc: 0.625 - ETA: 6s - loss: 1.5325 - acc: 0.618 - ETA: 6s - loss: 1.5652 - acc: 0.599 - ETA: 6s - loss: 1.5321 - acc: 0.609 - ETA: 6s - loss: 1.5245 - acc: 0.615 - ETA: 6s - loss: 1.5348 - acc: 0.620 - ETA: 6s - loss: 1.4972 - acc: 0.629 - ETA: 5s - loss: 1.4637 - acc: 0.640 - ETA: 5s - loss: 1.4316 - acc: 0.647 - ETA: 5s - loss: 1.4303 - acc: 0.645 - ETA: 5s - loss: 1.4344 - acc: 0.644 - ETA: 5s - loss: 1.4490 - acc: 0.642 - ETA: 5s - loss: 1.4514 - acc: 0.638 - ETA: 5s - loss: 1.4515 - acc: 0.637 - ETA: 5s - loss: 1.4457 - acc: 0.639 - ETA: 4s - loss: 1.4350 - acc: 0.640 - ETA: 4s - loss: 1.4165 - acc: 0.640 - ETA: 4s - loss: 1.4031 - acc: 0.642 - ETA: 4s - loss: 1.3990 - acc: 0.644 - ETA: 3s - loss: 1.4177 - acc: 0.641 - ETA: 3s - loss: 1.4081 - acc: 0.646 - ETA: 3s - loss: 1.4112 - acc: 0.646 - ETA: 3s - loss: 1.4165 - acc: 0.646 - ETA: 3s - loss: 1.4175 - acc: 0.645 - ETA: 3s - loss: 1.4093 - acc: 0.649 - ETA: 3s - loss: 1.4032 - acc: 0.652 - ETA: 2s - loss: 1.4064 - acc: 0.652 - ETA: 2s - loss: 1.4218 - acc: 0.647 - ETA: 2s - loss: 1.4101 - acc: 0.648 - ETA: 2s - loss: 1.4267 - acc: 0.644 - ETA: 2s - loss: 1.4308 - acc: 0.645 - ETA: 2s - loss: 1.4229 - acc: 0.646 - ETA: 2s - loss: 1.4345 - acc: 0.644 - ETA: 2s - loss: 1.4361 - acc: 0.641 - ETA: 2s - loss: 1.4310 - acc: 0.643 - ETA: 2s - loss: 1.4367 - acc: 0.641 - ETA: 2s - loss: 1.4338 - acc: 0.642 - ETA: 1s - loss: 1.4300 - acc: 0.642 - ETA: 1s - loss: 1.4255 - acc: 0.642 - ETA: 1s - loss: 1.4211 - acc: 0.643 - ETA: 1s - loss: 1.4213 - acc: 0.643 - ETA: 1s - loss: 1.4285 - acc: 0.641 - ETA: 1s - loss: 1.4401 - acc: 0.638 - ETA: 1s - loss: 1.4484 - acc: 0.636 - ETA: 1s - loss: 1.4395 - acc: 0.637 - ETA: 1s - loss: 1.4417 - acc: 0.638 - ETA: 1s - loss: 1.4414 - acc: 0.637 - ETA: 0s - loss: 1.4383 - acc: 0.638 - ETA: 0s - loss: 1.4379 - acc: 0.638 - ETA: 0s - loss: 1.4434 - acc: 0.637 - ETA: 0s - loss: 1.4362 - acc: 0.638 - ETA: 0s - loss: 1.4368 - acc: 0.637 - ETA: 0s - loss: 1.4345 - acc: 0.638 - ETA: 0s - loss: 1.4306 - acc: 0.639 - ETA: 0s - loss: 1.4264 - acc: 0.640 - ETA: 0s - loss: 1.4302 - acc: 0.638 - 8s 4ms/step - loss: 1.4286 - acc: 0.6380 - val_loss: 2.3914 - val_acc: 0.4677\n",
      "Epoch 5/10\n",
      "2108/2108 [==============================] - ETA: 6s - loss: 0.9713 - acc: 0.781 - ETA: 6s - loss: 1.1140 - acc: 0.718 - ETA: 6s - loss: 1.2106 - acc: 0.718 - ETA: 5s - loss: 1.1939 - acc: 0.726 - ETA: 5s - loss: 1.1248 - acc: 0.737 - ETA: 4s - loss: 1.1226 - acc: 0.739 - ETA: 4s - loss: 1.1418 - acc: 0.732 - ETA: 4s - loss: 1.1399 - acc: 0.730 - ETA: 4s - loss: 1.2032 - acc: 0.711 - ETA: 3s - loss: 1.1704 - acc: 0.718 - ETA: 3s - loss: 1.1867 - acc: 0.705 - ETA: 3s - loss: 1.1632 - acc: 0.704 - ETA: 3s - loss: 1.1411 - acc: 0.707 - ETA: 3s - loss: 1.1219 - acc: 0.710 - ETA: 3s - loss: 1.1011 - acc: 0.718 - ETA: 3s - loss: 1.1031 - acc: 0.713 - ETA: 3s - loss: 1.1089 - acc: 0.713 - ETA: 2s - loss: 1.0977 - acc: 0.720 - ETA: 2s - loss: 1.1020 - acc: 0.723 - ETA: 2s - loss: 1.1025 - acc: 0.721 - ETA: 2s - loss: 1.0920 - acc: 0.725 - ETA: 2s - loss: 1.0946 - acc: 0.725 - ETA: 2s - loss: 1.1173 - acc: 0.721 - ETA: 2s - loss: 1.1145 - acc: 0.723 - ETA: 2s - loss: 1.1056 - acc: 0.721 - ETA: 2s - loss: 1.0974 - acc: 0.721 - ETA: 2s - loss: 1.0914 - acc: 0.722 - ETA: 2s - loss: 1.0839 - acc: 0.723 - ETA: 2s - loss: 1.0901 - acc: 0.719 - ETA: 2s - loss: 1.0952 - acc: 0.716 - ETA: 2s - loss: 1.1004 - acc: 0.715 - ETA: 2s - loss: 1.1089 - acc: 0.713 - ETA: 1s - loss: 1.0919 - acc: 0.716 - ETA: 1s - loss: 1.0944 - acc: 0.715 - ETA: 1s - loss: 1.0868 - acc: 0.716 - ETA: 1s - loss: 1.0868 - acc: 0.715 - ETA: 1s - loss: 1.0864 - acc: 0.713 - ETA: 1s - loss: 1.0881 - acc: 0.711 - ETA: 1s - loss: 1.0797 - acc: 0.714 - ETA: 1s - loss: 1.0777 - acc: 0.713 - ETA: 1s - loss: 1.0747 - acc: 0.715 - ETA: 1s - loss: 1.0782 - acc: 0.715 - ETA: 1s - loss: 1.0795 - acc: 0.715 - ETA: 1s - loss: 1.0790 - acc: 0.716 - ETA: 1s - loss: 1.0784 - acc: 0.716 - ETA: 1s - loss: 1.0791 - acc: 0.716 - ETA: 1s - loss: 1.0827 - acc: 0.714 - ETA: 1s - loss: 1.0795 - acc: 0.716 - ETA: 1s - loss: 1.0773 - acc: 0.717 - ETA: 1s - loss: 1.0732 - acc: 0.717 - ETA: 0s - loss: 1.0764 - acc: 0.718 - ETA: 0s - loss: 1.0777 - acc: 0.715 - ETA: 0s - loss: 1.0748 - acc: 0.716 - ETA: 0s - loss: 1.0742 - acc: 0.717 - ETA: 0s - loss: 1.0810 - acc: 0.716 - ETA: 0s - loss: 1.0796 - acc: 0.716 - ETA: 0s - loss: 1.0810 - acc: 0.715 - ETA: 0s - loss: 1.0844 - acc: 0.713 - ETA: 0s - loss: 1.0837 - acc: 0.713 - ETA: 0s - loss: 1.0955 - acc: 0.711 - ETA: 0s - loss: 1.0954 - acc: 0.713 - ETA: 0s - loss: 1.1012 - acc: 0.711 - ETA: 0s - loss: 1.0977 - acc: 0.713 - ETA: 0s - loss: 1.0990 - acc: 0.712 - 6s 3ms/step - loss: 1.0940 - acc: 0.7130 - val_loss: 2.3335 - val_acc: 0.4973\n",
      "Epoch 6/10\n",
      "2108/2108 [==============================] - ETA: 6s - loss: 0.5137 - acc: 0.906 - ETA: 10s - loss: 0.7282 - acc: 0.79 - ETA: 8s - loss: 0.8489 - acc: 0.7708 - ETA: 7s - loss: 0.7431 - acc: 0.820 - ETA: 6s - loss: 0.7328 - acc: 0.806 - ETA: 5s - loss: 0.7213 - acc: 0.812 - ETA: 5s - loss: 0.7350 - acc: 0.808 - ETA: 5s - loss: 0.7664 - acc: 0.804 - ETA: 4s - loss: 0.7398 - acc: 0.812 - ETA: 4s - loss: 0.7655 - acc: 0.806 - ETA: 4s - loss: 0.7731 - acc: 0.804 - ETA: 4s - loss: 0.7558 - acc: 0.807 - ETA: 4s - loss: 0.7523 - acc: 0.807 - ETA: 3s - loss: 0.7479 - acc: 0.810 - ETA: 3s - loss: 0.7415 - acc: 0.810 - ETA: 3s - loss: 0.7362 - acc: 0.806 - ETA: 3s - loss: 0.7259 - acc: 0.807 - ETA: 3s - loss: 0.7568 - acc: 0.798 - ETA: 3s - loss: 0.7605 - acc: 0.801 - ETA: 3s - loss: 0.7854 - acc: 0.796 - ETA: 3s - loss: 0.7960 - acc: 0.791 - ETA: 3s - loss: 0.8058 - acc: 0.786 - ETA: 2s - loss: 0.8039 - acc: 0.782 - ETA: 2s - loss: 0.8068 - acc: 0.785 - ETA: 2s - loss: 0.8078 - acc: 0.785 - ETA: 2s - loss: 0.8339 - acc: 0.782 - ETA: 2s - loss: 0.8385 - acc: 0.781 - ETA: 2s - loss: 0.8548 - acc: 0.777 - ETA: 2s - loss: 0.8567 - acc: 0.776 - ETA: 2s - loss: 0.8589 - acc: 0.777 - ETA: 2s - loss: 0.8602 - acc: 0.776 - ETA: 2s - loss: 0.8561 - acc: 0.776 - ETA: 2s - loss: 0.8582 - acc: 0.773 - ETA: 2s - loss: 0.8602 - acc: 0.772 - ETA: 1s - loss: 0.8552 - acc: 0.771 - ETA: 1s - loss: 0.8530 - acc: 0.771 - ETA: 1s - loss: 0.8517 - acc: 0.772 - ETA: 1s - loss: 0.8573 - acc: 0.768 - ETA: 1s - loss: 0.8560 - acc: 0.767 - ETA: 1s - loss: 0.8583 - acc: 0.768 - ETA: 1s - loss: 0.8558 - acc: 0.769 - ETA: 1s - loss: 0.8514 - acc: 0.771 - ETA: 1s - loss: 0.8499 - acc: 0.773 - ETA: 1s - loss: 0.8476 - acc: 0.773 - ETA: 1s - loss: 0.8514 - acc: 0.772 - ETA: 1s - loss: 0.8485 - acc: 0.771 - ETA: 1s - loss: 0.8523 - acc: 0.770 - ETA: 1s - loss: 0.8534 - acc: 0.766 - ETA: 1s - loss: 0.8545 - acc: 0.764 - ETA: 1s - loss: 0.8566 - acc: 0.763 - ETA: 1s - loss: 0.8592 - acc: 0.762 - ETA: 1s - loss: 0.8553 - acc: 0.763 - ETA: 0s - loss: 0.8600 - acc: 0.762 - ETA: 0s - loss: 0.8685 - acc: 0.762 - ETA: 0s - loss: 0.8681 - acc: 0.761 - ETA: 0s - loss: 0.8699 - acc: 0.761 - ETA: 0s - loss: 0.8661 - acc: 0.763 - ETA: 0s - loss: 0.8663 - acc: 0.764 - ETA: 0s - loss: 0.8634 - acc: 0.764 - ETA: 0s - loss: 0.8631 - acc: 0.763 - ETA: 0s - loss: 0.8643 - acc: 0.763 - ETA: 0s - loss: 0.8664 - acc: 0.762 - ETA: 0s - loss: 0.8670 - acc: 0.759 - ETA: 0s - loss: 0.8634 - acc: 0.761 - ETA: 0s - loss: 0.8699 - acc: 0.760 - 6s 3ms/step - loss: 0.8698 - acc: 0.7614 - val_loss: 2.3700 - val_acc: 0.5403\n",
      "Epoch 7/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 6s - loss: 0.3846 - acc: 0.906 - ETA: 6s - loss: 0.5560 - acc: 0.875 - ETA: 5s - loss: 0.7676 - acc: 0.812 - ETA: 5s - loss: 0.7745 - acc: 0.804 - ETA: 4s - loss: 0.7018 - acc: 0.831 - ETA: 4s - loss: 0.7105 - acc: 0.833 - ETA: 4s - loss: 0.7308 - acc: 0.821 - ETA: 4s - loss: 0.7169 - acc: 0.828 - ETA: 4s - loss: 0.7340 - acc: 0.822 - ETA: 3s - loss: 0.7171 - acc: 0.825 - ETA: 3s - loss: 0.7388 - acc: 0.821 - ETA: 3s - loss: 0.7369 - acc: 0.820 - ETA: 3s - loss: 0.7330 - acc: 0.824 - ETA: 3s - loss: 0.7246 - acc: 0.825 - ETA: 3s - loss: 0.7034 - acc: 0.829 - ETA: 3s - loss: 0.7218 - acc: 0.824 - ETA: 3s - loss: 0.7064 - acc: 0.827 - ETA: 3s - loss: 0.6887 - acc: 0.831 - ETA: 3s - loss: 0.6869 - acc: 0.825 - ETA: 2s - loss: 0.6908 - acc: 0.823 - ETA: 2s - loss: 0.6934 - acc: 0.821 - ETA: 2s - loss: 0.6909 - acc: 0.821 - ETA: 2s - loss: 0.6761 - acc: 0.824 - ETA: 2s - loss: 0.6776 - acc: 0.824 - ETA: 2s - loss: 0.6850 - acc: 0.820 - ETA: 2s - loss: 0.6902 - acc: 0.818 - ETA: 2s - loss: 0.6935 - acc: 0.814 - ETA: 2s - loss: 0.6932 - acc: 0.813 - ETA: 2s - loss: 0.6959 - acc: 0.810 - ETA: 2s - loss: 0.6890 - acc: 0.812 - ETA: 2s - loss: 0.7033 - acc: 0.812 - ETA: 2s - loss: 0.7020 - acc: 0.812 - ETA: 2s - loss: 0.7119 - acc: 0.807 - ETA: 2s - loss: 0.7150 - acc: 0.804 - ETA: 2s - loss: 0.7093 - acc: 0.805 - ETA: 2s - loss: 0.7013 - acc: 0.809 - ETA: 2s - loss: 0.7023 - acc: 0.808 - ETA: 2s - loss: 0.7088 - acc: 0.806 - ETA: 2s - loss: 0.7097 - acc: 0.806 - ETA: 2s - loss: 0.7050 - acc: 0.807 - ETA: 2s - loss: 0.7038 - acc: 0.808 - ETA: 1s - loss: 0.6954 - acc: 0.811 - ETA: 1s - loss: 0.6921 - acc: 0.811 - ETA: 1s - loss: 0.6855 - acc: 0.814 - ETA: 1s - loss: 0.6867 - acc: 0.816 - ETA: 1s - loss: 0.6772 - acc: 0.819 - ETA: 1s - loss: 0.6781 - acc: 0.818 - ETA: 1s - loss: 0.6810 - acc: 0.818 - ETA: 1s - loss: 0.6804 - acc: 0.818 - ETA: 1s - loss: 0.6749 - acc: 0.819 - ETA: 1s - loss: 0.6714 - acc: 0.819 - ETA: 1s - loss: 0.6726 - acc: 0.819 - ETA: 1s - loss: 0.6749 - acc: 0.819 - ETA: 1s - loss: 0.6685 - acc: 0.820 - ETA: 1s - loss: 0.6688 - acc: 0.821 - ETA: 0s - loss: 0.6694 - acc: 0.822 - ETA: 0s - loss: 0.6728 - acc: 0.820 - ETA: 0s - loss: 0.6725 - acc: 0.820 - ETA: 0s - loss: 0.6752 - acc: 0.818 - ETA: 0s - loss: 0.6708 - acc: 0.818 - ETA: 0s - loss: 0.6682 - acc: 0.819 - ETA: 0s - loss: 0.6687 - acc: 0.819 - ETA: 0s - loss: 0.6714 - acc: 0.819 - ETA: 0s - loss: 0.6737 - acc: 0.818 - ETA: 0s - loss: 0.6704 - acc: 0.818 - 6s 3ms/step - loss: 0.6685 - acc: 0.8188 - val_loss: 2.4732 - val_acc: 0.5188\n",
      "Epoch 8/10\n",
      "2108/2108 [==============================] - ETA: 6s - loss: 0.4030 - acc: 0.937 - ETA: 7s - loss: 0.5092 - acc: 0.890 - ETA: 7s - loss: 0.4559 - acc: 0.895 - ETA: 7s - loss: 0.4166 - acc: 0.890 - ETA: 7s - loss: 0.4146 - acc: 0.887 - ETA: 7s - loss: 0.3724 - acc: 0.906 - ETA: 6s - loss: 0.3621 - acc: 0.910 - ETA: 6s - loss: 0.3838 - acc: 0.890 - ETA: 6s - loss: 0.3987 - acc: 0.881 - ETA: 6s - loss: 0.4423 - acc: 0.865 - ETA: 6s - loss: 0.4253 - acc: 0.875 - ETA: 6s - loss: 0.4017 - acc: 0.882 - ETA: 6s - loss: 0.4036 - acc: 0.884 - ETA: 6s - loss: 0.3875 - acc: 0.890 - ETA: 6s - loss: 0.3876 - acc: 0.887 - ETA: 5s - loss: 0.3857 - acc: 0.888 - ETA: 5s - loss: 0.4166 - acc: 0.878 - ETA: 5s - loss: 0.4180 - acc: 0.875 - ETA: 5s - loss: 0.4153 - acc: 0.878 - ETA: 5s - loss: 0.4102 - acc: 0.881 - ETA: 5s - loss: 0.4415 - acc: 0.870 - ETA: 5s - loss: 0.4418 - acc: 0.869 - ETA: 5s - loss: 0.4504 - acc: 0.870 - ETA: 5s - loss: 0.4547 - acc: 0.868 - ETA: 4s - loss: 0.4708 - acc: 0.868 - ETA: 4s - loss: 0.4773 - acc: 0.866 - ETA: 4s - loss: 0.4782 - acc: 0.864 - ETA: 4s - loss: 0.4790 - acc: 0.865 - ETA: 3s - loss: 0.4857 - acc: 0.861 - ETA: 3s - loss: 0.4916 - acc: 0.858 - ETA: 3s - loss: 0.5038 - acc: 0.855 - ETA: 3s - loss: 0.5015 - acc: 0.857 - ETA: 3s - loss: 0.4941 - acc: 0.858 - ETA: 3s - loss: 0.4913 - acc: 0.858 - ETA: 3s - loss: 0.4892 - acc: 0.858 - ETA: 2s - loss: 0.4829 - acc: 0.860 - ETA: 2s - loss: 0.4756 - acc: 0.861 - ETA: 2s - loss: 0.4714 - acc: 0.860 - ETA: 2s - loss: 0.4732 - acc: 0.859 - ETA: 2s - loss: 0.4718 - acc: 0.858 - ETA: 2s - loss: 0.4702 - acc: 0.860 - ETA: 2s - loss: 0.4690 - acc: 0.860 - ETA: 2s - loss: 0.4704 - acc: 0.861 - ETA: 2s - loss: 0.4738 - acc: 0.860 - ETA: 2s - loss: 0.4914 - acc: 0.858 - ETA: 2s - loss: 0.4957 - acc: 0.858 - ETA: 1s - loss: 0.4996 - acc: 0.857 - ETA: 1s - loss: 0.4946 - acc: 0.858 - ETA: 1s - loss: 0.4939 - acc: 0.857 - ETA: 1s - loss: 0.5004 - acc: 0.856 - ETA: 1s - loss: 0.4973 - acc: 0.857 - ETA: 1s - loss: 0.5032 - acc: 0.856 - ETA: 1s - loss: 0.5038 - acc: 0.856 - ETA: 1s - loss: 0.5054 - acc: 0.854 - ETA: 1s - loss: 0.5069 - acc: 0.854 - ETA: 1s - loss: 0.5151 - acc: 0.852 - ETA: 0s - loss: 0.5194 - acc: 0.851 - ETA: 0s - loss: 0.5180 - acc: 0.851 - ETA: 0s - loss: 0.5157 - acc: 0.851 - ETA: 0s - loss: 0.5202 - acc: 0.850 - ETA: 0s - loss: 0.5212 - acc: 0.849 - ETA: 0s - loss: 0.5207 - acc: 0.848 - ETA: 0s - loss: 0.5289 - acc: 0.847 - ETA: 0s - loss: 0.5277 - acc: 0.848 - 7s 3ms/step - loss: 0.5278 - acc: 0.8482 - val_loss: 2.5689 - val_acc: 0.5296\n",
      "Epoch 9/10\n",
      "2108/2108 [==============================] - ETA: 14s - loss: 0.4112 - acc: 0.84 - ETA: 11s - loss: 0.2863 - acc: 0.89 - ETA: 9s - loss: 0.3940 - acc: 0.8646 - ETA: 9s - loss: 0.3646 - acc: 0.882 - ETA: 8s - loss: 0.4202 - acc: 0.868 - ETA: 8s - loss: 0.3969 - acc: 0.875 - ETA: 8s - loss: 0.4070 - acc: 0.866 - ETA: 7s - loss: 0.4272 - acc: 0.863 - ETA: 7s - loss: 0.4130 - acc: 0.871 - ETA: 7s - loss: 0.4188 - acc: 0.868 - ETA: 7s - loss: 0.4227 - acc: 0.869 - ETA: 6s - loss: 0.4049 - acc: 0.877 - ETA: 6s - loss: 0.3982 - acc: 0.882 - ETA: 6s - loss: 0.4279 - acc: 0.881 - ETA: 6s - loss: 0.4358 - acc: 0.879 - ETA: 6s - loss: 0.4289 - acc: 0.878 - ETA: 6s - loss: 0.4173 - acc: 0.884 - ETA: 6s - loss: 0.4254 - acc: 0.881 - ETA: 5s - loss: 0.4243 - acc: 0.881 - ETA: 5s - loss: 0.4140 - acc: 0.885 - ETA: 5s - loss: 0.4161 - acc: 0.885 - ETA: 5s - loss: 0.4141 - acc: 0.887 - ETA: 5s - loss: 0.4034 - acc: 0.889 - ETA: 5s - loss: 0.4008 - acc: 0.890 - ETA: 5s - loss: 0.4160 - acc: 0.888 - ETA: 5s - loss: 0.4153 - acc: 0.887 - ETA: 4s - loss: 0.4091 - acc: 0.890 - ETA: 4s - loss: 0.4052 - acc: 0.891 - ETA: 4s - loss: 0.4123 - acc: 0.889 - ETA: 4s - loss: 0.4104 - acc: 0.887 - ETA: 4s - loss: 0.4134 - acc: 0.886 - ETA: 3s - loss: 0.4205 - acc: 0.879 - ETA: 3s - loss: 0.4187 - acc: 0.880 - ETA: 3s - loss: 0.4156 - acc: 0.881 - ETA: 3s - loss: 0.4117 - acc: 0.883 - ETA: 3s - loss: 0.4126 - acc: 0.881 - ETA: 3s - loss: 0.4091 - acc: 0.881 - ETA: 3s - loss: 0.4131 - acc: 0.878 - ETA: 3s - loss: 0.4182 - acc: 0.875 - ETA: 2s - loss: 0.4142 - acc: 0.877 - ETA: 2s - loss: 0.4163 - acc: 0.876 - ETA: 2s - loss: 0.4174 - acc: 0.876 - ETA: 2s - loss: 0.4235 - acc: 0.875 - ETA: 2s - loss: 0.4169 - acc: 0.877 - ETA: 2s - loss: 0.4194 - acc: 0.876 - ETA: 2s - loss: 0.4144 - acc: 0.878 - ETA: 2s - loss: 0.4153 - acc: 0.878 - ETA: 2s - loss: 0.4229 - acc: 0.874 - ETA: 1s - loss: 0.4287 - acc: 0.872 - ETA: 1s - loss: 0.4288 - acc: 0.871 - ETA: 1s - loss: 0.4314 - acc: 0.871 - ETA: 1s - loss: 0.4323 - acc: 0.871 - ETA: 1s - loss: 0.4328 - acc: 0.871 - ETA: 1s - loss: 0.4352 - acc: 0.870 - ETA: 1s - loss: 0.4302 - acc: 0.872 - ETA: 1s - loss: 0.4327 - acc: 0.871 - ETA: 1s - loss: 0.4287 - acc: 0.873 - ETA: 0s - loss: 0.4412 - acc: 0.869 - ETA: 0s - loss: 0.4461 - acc: 0.867 - ETA: 0s - loss: 0.4475 - acc: 0.867 - ETA: 0s - loss: 0.4468 - acc: 0.867 - ETA: 0s - loss: 0.4454 - acc: 0.867 - ETA: 0s - loss: 0.4444 - acc: 0.868 - ETA: 0s - loss: 0.4454 - acc: 0.868 - ETA: 0s - loss: 0.4429 - acc: 0.869 - 8s 4ms/step - loss: 0.4429 - acc: 0.8691 - val_loss: 2.7843 - val_acc: 0.5242\n",
      "Epoch 10/10\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2108/2108 [==============================] - ETA: 2s - loss: 0.2338 - acc: 0.937 - ETA: 3s - loss: 0.1808 - acc: 0.953 - ETA: 3s - loss: 0.2121 - acc: 0.937 - ETA: 3s - loss: 0.3148 - acc: 0.921 - ETA: 2s - loss: 0.3004 - acc: 0.921 - ETA: 2s - loss: 0.2986 - acc: 0.919 - ETA: 2s - loss: 0.3058 - acc: 0.910 - ETA: 2s - loss: 0.3324 - acc: 0.899 - ETA: 3s - loss: 0.3272 - acc: 0.893 - ETA: 3s - loss: 0.3223 - acc: 0.894 - ETA: 4s - loss: 0.3223 - acc: 0.893 - ETA: 4s - loss: 0.3240 - acc: 0.894 - ETA: 4s - loss: 0.3219 - acc: 0.895 - ETA: 4s - loss: 0.3182 - acc: 0.895 - ETA: 4s - loss: 0.3281 - acc: 0.894 - ETA: 4s - loss: 0.3373 - acc: 0.897 - ETA: 4s - loss: 0.3443 - acc: 0.897 - ETA: 4s - loss: 0.3446 - acc: 0.898 - ETA: 4s - loss: 0.3494 - acc: 0.895 - ETA: 4s - loss: 0.3476 - acc: 0.895 - ETA: 4s - loss: 0.3774 - acc: 0.893 - ETA: 4s - loss: 0.3715 - acc: 0.895 - ETA: 4s - loss: 0.3759 - acc: 0.891 - ETA: 4s - loss: 0.3732 - acc: 0.893 - ETA: 3s - loss: 0.3746 - acc: 0.893 - ETA: 3s - loss: 0.3715 - acc: 0.894 - ETA: 3s - loss: 0.3701 - acc: 0.895 - ETA: 3s - loss: 0.3673 - acc: 0.896 - ETA: 3s - loss: 0.3684 - acc: 0.896 - ETA: 3s - loss: 0.3648 - acc: 0.897 - ETA: 3s - loss: 0.3630 - acc: 0.898 - ETA: 3s - loss: 0.3552 - acc: 0.900 - ETA: 3s - loss: 0.3546 - acc: 0.901 - ETA: 3s - loss: 0.3525 - acc: 0.901 - ETA: 3s - loss: 0.3539 - acc: 0.902 - ETA: 3s - loss: 0.3507 - acc: 0.902 - ETA: 2s - loss: 0.3499 - acc: 0.903 - ETA: 2s - loss: 0.3444 - acc: 0.904 - ETA: 2s - loss: 0.3397 - acc: 0.905 - ETA: 2s - loss: 0.3400 - acc: 0.905 - ETA: 2s - loss: 0.3380 - acc: 0.905 - ETA: 2s - loss: 0.3368 - acc: 0.904 - ETA: 1s - loss: 0.3363 - acc: 0.904 - ETA: 1s - loss: 0.3350 - acc: 0.904 - ETA: 1s - loss: 0.3401 - acc: 0.902 - ETA: 1s - loss: 0.3407 - acc: 0.903 - ETA: 1s - loss: 0.3371 - acc: 0.904 - ETA: 1s - loss: 0.3372 - acc: 0.904 - ETA: 1s - loss: 0.3390 - acc: 0.903 - ETA: 1s - loss: 0.3362 - acc: 0.903 - ETA: 1s - loss: 0.3323 - acc: 0.905 - ETA: 1s - loss: 0.3318 - acc: 0.904 - ETA: 1s - loss: 0.3312 - acc: 0.905 - ETA: 0s - loss: 0.3305 - acc: 0.905 - ETA: 0s - loss: 0.3287 - acc: 0.905 - ETA: 0s - loss: 0.3271 - acc: 0.905 - ETA: 0s - loss: 0.3310 - acc: 0.903 - ETA: 0s - loss: 0.3307 - acc: 0.902 - ETA: 0s - loss: 0.3302 - acc: 0.901 - ETA: 0s - loss: 0.3353 - acc: 0.900 - ETA: 0s - loss: 0.3354 - acc: 0.901 - ETA: 0s - loss: 0.3347 - acc: 0.901 - ETA: 0s - loss: 0.3363 - acc: 0.901 - 7s 3ms/step - loss: 0.3345 - acc: 0.9023 - val_loss: 2.8091 - val_acc: 0.5376\n",
      "Final accuracy on the training data is: 0.9022770400744009\n",
      "Cross_Validating accuracy is: 0.5376344073203302\n"
     ]
    }
   ],
   "source": [
    "ganguly = Sequential()\n",
    "ganguly.add(Conv2D(248, kernel_size=5, activation='relu',border_mode='same', input_shape=(h,w,d)))\n",
    "ganguly.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ganguly.add(Conv2D(62, kernel_size=3, activation='relu',border_mode='same'))\n",
    "ganguly.add(MaxPooling2D(pool_size=(2,2)))\n",
    "ganguly.add(Flatten())\n",
    "ganguly.add(Dense(62, activation='softmax'))\n",
    "ganguly.compile(loss='categorical_crossentropy',\n",
    "              optimizer='adam',\n",
    "              metrics=['accuracy'])\n",
    "feeds=ganguly.fit(x=X_train_arr,y=y_train_arr,epochs=10,validation_data=(X_val_arr,y_val_arr))\n",
    "print(\"Final accuracy on the training data is:\",feeds.history['acc'][-1]);\n",
    "print(\"Cross_Validating accuracy is:\",feeds.history['val_acc'][-1]);"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "thus at the end we stick with Model 2.\n",
    "..........End of subpart a........."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
